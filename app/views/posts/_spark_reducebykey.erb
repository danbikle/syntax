<code class='python'>

mylist = [ {'k1': 1}, {'k2': 2}, {'k1': -2}, {'k3': 4}, {'k2': -5}, {'k1': 4} ]

myrdd = sc.parallelize(mylist)

myrdd.take(6)

mylam = lambda v1,v2: v1+v2

reduced_rdd = myrdd.reduceByKey(mylam)

fails: reduced_rdd.count()
fails: reduced_rdd.take(2)


try this:

mylist2 = [ ('k1', 1), ('k2', 2), ('k1', -2), ('k3', 4), ('k2', -5), ('k1', 4) ]
myrdd2 = sc.parallelize(mylist2)

myrdd2.count()

myrdd2.take(6)
reduced_rdd2 = myrdd2.reduceByKey(mylam)
reduced_rdd2.count()
reduced_rdd2.take(reduced_rdd2.count())

Try this:

mylist3 = [ ['k1', 1], ['k2', 2], ['k1', -2], ['k3', 4], ['k2', -5], ['k1', 4] ]
myrdd3 = sc.parallelize(mylist3)

myrdd3.count()

myrdd3.take(6)
reduced_rdd3 = myrdd3.reduceByKey(mylam)
reduced_rdd3.count()
reduced_rdd3.take(reduced_rdd3.count())


dan@feb ~/spark $ 
dan@feb ~/spark $ bin/pyspark
Python 2.7.8 |Anaconda 2.1.0 (64-bit)| (default, Aug 21 2014, 18:22:21) 
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2
Type "help", "copyright", "credits" or "license" for more information.
Anaconda is brought to you by Continuum Analytics.
Please check out: http://continuum.io/thanks and https://binstar.org
Spark assembly has been built with Hive, including Datanucleus jars on classpath
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
15/02/01 22:52:00 WARN Utils: Your hostname, feb resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface eth0)
15/02/01 22:52:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
15/02/01 22:52:01 INFO SecurityManager: Changing view acls to: dan
15/02/01 22:52:01 INFO SecurityManager: Changing modify acls to: dan
15/02/01 22:52:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(dan); users with modify permissions: Set(dan)
15/02/01 22:52:02 INFO Slf4jLogger: Slf4jLogger started
15/02/01 22:52:02 INFO Remoting: Starting remoting
15/02/01 22:52:02 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.0.2.15:45544]
15/02/01 22:52:02 INFO Utils: Successfully started service 'sparkDriver' on port 45544.
15/02/01 22:52:02 INFO SparkEnv: Registering MapOutputTracker
15/02/01 22:52:02 INFO SparkEnv: Registering BlockManagerMaster
15/02/01 22:52:02 INFO DiskBlockManager: Created local directory at /tmp/spark-local-20150201225202-4305
15/02/01 22:52:02 INFO MemoryStore: MemoryStore started with capacity 265.4 MB
15/02/01 22:52:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/02/01 22:52:03 INFO HttpFileServer: HTTP File server directory is /tmp/spark-f8ba9c17-bf35-4bfd-9573-e88917aa4a58
15/02/01 22:52:03 INFO HttpServer: Starting HTTP Server
15/02/01 22:52:04 INFO Utils: Successfully started service 'HTTP file server' on port 41382.
15/02/01 22:52:04 INFO Utils: Successfully started service 'SparkUI' on port 4040.
15/02/01 22:52:04 INFO SparkUI: Started SparkUI at http://10.0.2.15:4040
15/02/01 22:52:04 INFO AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@10.0.2.15:45544/user/HeartbeatReceiver
15/02/01 22:52:04 INFO NettyBlockTransferService: Server created on 43784
15/02/01 22:52:04 INFO BlockManagerMaster: Trying to register BlockManager
15/02/01 22:52:04 INFO BlockManagerMasterActor: Registering block manager localhost:43784 with 265.4 MB RAM, BlockManagerId(<driver>, localhost, 43784)
15/02/01 22:52:04 INFO BlockManagerMaster: Registered BlockManager
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 1.2.0
      /_/

Using Python version 2.7.8 (default, Aug 21 2014 18:22:21)
SparkContext available as sc.
>>> 
>>> mylist = [ {'k1': 1}, {'k2': 2}, {'k1': -2}, {'k3': 4}, {'k2': -5}, {'k1': 4} ]
>>> myrdd = sc.parallelize(mylist)
>>> myrdd.take(6)
15/02/01 22:52:37 INFO SparkContext: Starting job: runJob at PythonRDD.scala:344
15/02/01 22:52:37 INFO DAGScheduler: Got job 0 (runJob at PythonRDD.scala:344) with 1 output partitions (allowLocal=true)
15/02/01 22:52:37 INFO DAGScheduler: Final stage: Stage 0(runJob at PythonRDD.scala:344)
15/02/01 22:52:37 INFO DAGScheduler: Parents of final stage: List()
15/02/01 22:52:37 INFO DAGScheduler: Missing parents: List()
15/02/01 22:52:37 INFO DAGScheduler: Submitting Stage 0 (PythonRDD[1] at RDD at PythonRDD.scala:43), which has no missing parents
15/02/01 22:52:38 INFO MemoryStore: ensureFreeSpace(3440) called with curMem=0, maxMem=278302556
15/02/01 22:52:38 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 3.4 KB, free 265.4 MB)
15/02/01 22:52:38 INFO MemoryStore: ensureFreeSpace(2587) called with curMem=3440, maxMem=278302556
15/02/01 22:52:38 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.5 KB, free 265.4 MB)
15/02/01 22:52:38 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:43784 (size: 2.5 KB, free: 265.4 MB)
15/02/01 22:52:38 INFO BlockManagerMaster: Updated info of block broadcast_0_piece0
15/02/01 22:52:38 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:838
15/02/01 22:52:38 INFO DAGScheduler: Submitting 1 missing tasks from Stage 0 (PythonRDD[1] at RDD at PythonRDD.scala:43)
15/02/01 22:52:38 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
15/02/01 22:52:38 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1304 bytes)
15/02/01 22:52:38 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
15/02/01 22:52:39 INFO PythonRDD: Times: total = 530, boot = 495, init = 35, finish = 0
15/02/01 22:52:39 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 724 bytes result sent to driver
15/02/01 22:52:39 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 660 ms on localhost (1/1)
15/02/01 22:52:39 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
15/02/01 22:52:39 INFO DAGScheduler: Stage 0 (runJob at PythonRDD.scala:344) finished in 0.701 s
15/02/01 22:52:39 INFO DAGScheduler: Job 0 finished: runJob at PythonRDD.scala:344, took 1.400996 s
15/02/01 22:52:39 INFO SparkContext: Starting job: runJob at PythonRDD.scala:344
15/02/01 22:52:39 INFO DAGScheduler: Got job 1 (runJob at PythonRDD.scala:344) with 2 output partitions (allowLocal=true)
15/02/01 22:52:39 INFO DAGScheduler: Final stage: Stage 1(runJob at PythonRDD.scala:344)
15/02/01 22:52:39 INFO DAGScheduler: Parents of final stage: List()
15/02/01 22:52:39 INFO DAGScheduler: Missing parents: List()
15/02/01 22:52:39 INFO DAGScheduler: Submitting Stage 1 (PythonRDD[2] at RDD at PythonRDD.scala:43), which has no missing parents
15/02/01 22:52:39 INFO MemoryStore: ensureFreeSpace(3440) called with curMem=6027, maxMem=278302556
15/02/01 22:52:39 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.4 KB, free 265.4 MB)
15/02/01 22:52:39 INFO MemoryStore: ensureFreeSpace(2587) called with curMem=9467, maxMem=278302556
15/02/01 22:52:39 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KB, free 265.4 MB)
15/02/01 22:52:39 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:43784 (size: 2.5 KB, free: 265.4 MB)
15/02/01 22:52:39 INFO BlockManagerMaster: Updated info of block broadcast_1_piece0
15/02/01 22:52:39 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:838
15/02/01 22:52:39 INFO DAGScheduler: Submitting 2 missing tasks from Stage 1 (PythonRDD[2] at RDD at PythonRDD.scala:43)
15/02/01 22:52:39 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
15/02/01 22:52:39 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, PROCESS_LOCAL, 1305 bytes)
15/02/01 22:52:39 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2, localhost, PROCESS_LOCAL, 1307 bytes)
15/02/01 22:52:39 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
15/02/01 22:52:39 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
15/02/01 22:52:39 INFO PythonRDD: Times: total = 44, boot = -147, init = 191, finish = 0
15/02/01 22:52:39 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 727 bytes result sent to driver
15/02/01 22:52:39 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 78 ms on localhost (1/2)
15/02/01 22:52:39 INFO PythonRDD: Times: total = 15, boot = 8, init = 5, finish = 2
15/02/01 22:52:39 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 727 bytes result sent to driver
15/02/01 22:52:39 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 121 ms on localhost (2/2)
15/02/01 22:52:39 INFO DAGScheduler: Stage 1 (runJob at PythonRDD.scala:344) finished in 0.125 s
15/02/01 22:52:39 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
15/02/01 22:52:39 INFO DAGScheduler: Job 1 finished: runJob at PythonRDD.scala:344, took 0.154482 s
[{'k1': 1}, {'k2': 2}, {'k1': -2}, {'k3': 4}, {'k2': -5}, {'k1': 4}]
>>> 
>>> 
>>> mylam = lambda v1,v2: v1+v2
>>> 
>>> reduced_rdd = myrdd.reduceByKey(mylam)
>>> 
>>> 
>>> reduced_rdd.count()
15/02/01 22:53:13 INFO SparkContext: Starting job: count at <stdin>:1
15/02/01 22:53:13 INFO DAGScheduler: Registering RDD 4 (reduceByKey at <stdin>:1)
15/02/01 22:53:13 INFO DAGScheduler: Got job 2 (count at <stdin>:1) with 3 output partitions (allowLocal=false)
15/02/01 22:53:13 INFO DAGScheduler: Final stage: Stage 3(count at <stdin>:1)
15/02/01 22:53:13 INFO DAGScheduler: Parents of final stage: List(Stage 2)
15/02/01 22:53:13 INFO DAGScheduler: Missing parents: List(Stage 2)
15/02/01 22:53:13 INFO DAGScheduler: Submitting Stage 2 (PairwiseRDD[4] at reduceByKey at <stdin>:1), which has no missing parents
15/02/01 22:53:13 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=12054, maxMem=278302556
15/02/01 22:53:13 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 5.9 KB, free 265.4 MB)
15/02/01 22:53:13 INFO MemoryStore: ensureFreeSpace(4653) called with curMem=18094, maxMem=278302556
15/02/01 22:53:13 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.5 KB, free 265.4 MB)
15/02/01 22:53:13 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:43784 (size: 4.5 KB, free: 265.4 MB)
15/02/01 22:53:13 INFO BlockManagerMaster: Updated info of block broadcast_2_piece0
15/02/01 22:53:13 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:838
15/02/01 22:53:13 INFO DAGScheduler: Submitting 3 missing tasks from Stage 2 (PairwiseRDD[4] at reduceByKey at <stdin>:1)
15/02/01 22:53:13 INFO TaskSchedulerImpl: Adding task set 2.0 with 3 tasks
15/02/01 22:53:13 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 3, localhost, PROCESS_LOCAL, 1293 bytes)
15/02/01 22:53:13 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 4, localhost, PROCESS_LOCAL, 1294 bytes)
15/02/01 22:53:13 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 5, localhost, PROCESS_LOCAL, 1296 bytes)
15/02/01 22:53:13 INFO Executor: Running task 0.0 in stage 2.0 (TID 3)
15/02/01 22:53:13 INFO Executor: Running task 1.0 in stage 2.0 (TID 4)
15/02/01 22:53:13 INFO Executor: Running task 2.0 in stage 2.0 (TID 5)
15/02/01 22:53:13 ERROR Executor: Exception in task 2.0 in stage 2.0 (TID 5)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/dan/spark/python/pyspark/worker.py", line 107, in main
    process()
  File "/home/dan/spark/python/pyspark/worker.py", line 98, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/dan/spark/python/pyspark/rdd.py", line 2073, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/home/dan/spark/python/pyspark/rdd.py", line 247, in func
    return f(iterator)
  File "/home/dan/spark/python/pyspark/rdd.py", line 1561, in combineLocally
    merger.mergeValues(iterator)
  File "/home/dan/spark/python/pyspark/shuffle.py", line 252, in mergeValues
    for k, v in iterator:
ValueError: need more than 1 value to unpack

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:137)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:174)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:96)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:305)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/02/01 22:53:13 ERROR Executor: Exception in task 1.0 in stage 2.0 (TID 4)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/dan/spark/python/pyspark/worker.py", line 107, in main
    process()
  File "/home/dan/spark/python/pyspark/worker.py", line 98, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/dan/spark/python/pyspark/rdd.py", line 2073, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/home/dan/spark/python/pyspark/rdd.py", line 247, in func
    return f(iterator)
  File "/home/dan/spark/python/pyspark/rdd.py", line 1561, in combineLocally
    merger.mergeValues(iterator)
  File "/home/dan/spark/python/pyspark/shuffle.py", line 252, in mergeValues
    for k, v in iterator:
ValueError: need more than 1 value to unpack

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:137)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:174)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:96)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:305)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/02/01 22:53:13 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 3)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/dan/spark/python/pyspark/worker.py", line 107, in main
    process()
  File "/home/dan/spark/python/pyspark/worker.py", line 98, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/dan/spark/python/pyspark/rdd.py", line 2073, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/home/dan/spark/python/pyspark/rdd.py", line 247, in func
    return f(iterator)
  File "/home/dan/spark/python/pyspark/rdd.py", line 1561, in combineLocally
    merger.mergeValues(iterator)
  File "/home/dan/spark/python/pyspark/shuffle.py", line 252, in mergeValues
    for k, v in iterator:
ValueError: need more than 1 value to unpack

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:137)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:174)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:96)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:305)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/02/01 22:53:13 WARN TaskSetManager: Lost task 1.0 in stage 2.0 (TID 4, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/dan/spark/python/pyspark/worker.py", line 107, in main
    process()
  File "/home/dan/spark/python/pyspark/worker.py", line 98, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/dan/spark/python/pyspark/rdd.py", line 2073, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/home/dan/spark/python/pyspark/rdd.py", line 247, in func
    return f(iterator)
  File "/home/dan/spark/python/pyspark/rdd.py", line 1561, in combineLocally
    merger.mergeValues(iterator)
  File "/home/dan/spark/python/pyspark/shuffle.py", line 252, in mergeValues
    for k, v in iterator:
ValueError: need more than 1 value to unpack

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:137)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:174)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:96)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:305)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

15/02/01 22:53:13 ERROR TaskSetManager: Task 1 in stage 2.0 failed 1 times; aborting job
15/02/01 22:53:13 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
15/02/01 22:53:13 INFO TaskSetManager: Lost task 0.0 in stage 2.0 (TID 3) on executor localhost: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/dan/spark/python/pyspark/worker.py", line 107, in main
    process()
  File "/home/dan/spark/python/pyspark/worker.py", line 98, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/dan/spark/python/pyspark/rdd.py", line 2073, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/home/dan/spark/python/pyspark/rdd.py", line 247, in func
    return f(iterator)
  File "/home/dan/spark/python/pyspark/rdd.py", line 1561, in combineLocally
    merger.mergeValues(iterator)
  File "/home/dan/spark/python/pyspark/shuffle.py", line 252, in mergeValues
    for k, v in iterator:
ValueError: need more than 1 value to unpack
) [duplicate 1]
15/02/01 22:53:13 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
15/02/01 22:53:13 INFO TaskSetManager: Lost task 2.0 in stage 2.0 (TID 5) on executor localhost: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/home/dan/spark/python/pyspark/worker.py", line 107, in main
    process()
  File "/home/dan/spark/python/pyspark/worker.py", line 98, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/dan/spark/python/pyspark/rdd.py", line 2073, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/home/dan/spark/python/pyspark/rdd.py", line 247, in func
    return f(iterator)
  File "/home/dan/spark/python/pyspark/rdd.py", line 1561, in combineLocally
    merger.mergeValues(iterator)
  File "/home/dan/spark/python/pyspark/shuffle.py", line 252, in mergeValues
    for k, v in iterator:
ValueError: need more than 1 value to unpack
) [duplicate 2]
15/02/01 22:53:13 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
15/02/01 22:53:13 INFO TaskSchedulerImpl: Cancelling stage 2
15/02/01 22:53:13 INFO DAGScheduler: Job 2 failed: count at <stdin>:1, took 0.215547 s
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/dan/spark/python/pyspark/rdd.py", line 819, in count
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "/home/dan/spark/python/pyspark/rdd.py", line 810, in sum
    return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
  File "/home/dan/spark/python/pyspark/rdd.py", line 715, in reduce
    vals = self.mapPartitions(func).collect()
  File "/home/dan/spark/python/pyspark/rdd.py", line 676, in collect
    bytesInJava = self._jrdd.collect().iterator()
  File "/home/dan/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py", line 538, in __call__
  File "/home/dan/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py", line 300, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o55.collect.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 2.0 failed 1 times, most recent failure: Lost task 1.0 in stage 2.0 (TID 4, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/dan/spark/python/pyspark/worker.py", line 107, in main
    process()
  File "/home/dan/spark/python/pyspark/worker.py", line 98, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/dan/spark/python/pyspark/rdd.py", line 2073, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/home/dan/spark/python/pyspark/rdd.py", line 247, in func
    return f(iterator)
  File "/home/dan/spark/python/pyspark/rdd.py", line 1561, in combineLocally
    merger.mergeValues(iterator)
  File "/home/dan/spark/python/pyspark/shuffle.py", line 252, in mergeValues
    for k, v in iterator:
ValueError: need more than 1 value to unpack

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:137)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:174)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:96)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:305)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1214)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1203)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1202)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1202)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:696)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1420)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:465)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor.aroundReceive(DAGScheduler.scala:1375)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
	at akka.actor.ActorCell.invoke(ActorCell.scala:487)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
	at akka.dispatch.Mailbox.run(Mailbox.scala:220)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

>>> 
>>> 
>>> mylist2 = [ ('k1', 1), ('k2', 2), ('k1', -2), ('k3', 4), ('k2', -5), ('k1', 4) ]
>>> 
>>> myrdd2 = sc.parallelize(mylist2)
>>> 
>>> myrdd2.count()
15/02/01 22:53:47 INFO SparkContext: Starting job: count at <stdin>:1
15/02/01 22:53:47 INFO DAGScheduler: Got job 3 (count at <stdin>:1) with 3 output partitions (allowLocal=false)
15/02/01 22:53:47 INFO DAGScheduler: Final stage: Stage 4(count at <stdin>:1)
15/02/01 22:53:47 INFO DAGScheduler: Parents of final stage: List()
15/02/01 22:53:47 INFO DAGScheduler: Missing parents: List()
15/02/01 22:53:47 INFO DAGScheduler: Submitting Stage 4 (PythonRDD[9] at count at <stdin>:1), which has no missing parents
15/02/01 22:53:47 INFO MemoryStore: ensureFreeSpace(4192) called with curMem=22747, maxMem=278302556
15/02/01 22:53:47 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 4.1 KB, free 265.4 MB)
15/02/01 22:53:47 INFO MemoryStore: ensureFreeSpace(3147) called with curMem=26939, maxMem=278302556
15/02/01 22:53:47 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.1 KB, free 265.4 MB)
15/02/01 22:53:47 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:43784 (size: 3.1 KB, free: 265.4 MB)
15/02/01 22:53:47 INFO BlockManagerMaster: Updated info of block broadcast_3_piece0
15/02/01 22:53:47 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:838
15/02/01 22:53:47 INFO DAGScheduler: Submitting 3 missing tasks from Stage 4 (PythonRDD[9] at count at <stdin>:1)
15/02/01 22:53:47 INFO TaskSchedulerImpl: Adding task set 4.0 with 3 tasks
15/02/01 22:53:47 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 6, localhost, PROCESS_LOCAL, 1302 bytes)
15/02/01 22:53:47 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 7, localhost, PROCESS_LOCAL, 1303 bytes)
15/02/01 22:53:47 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 8, localhost, PROCESS_LOCAL, 1305 bytes)
15/02/01 22:53:47 INFO Executor: Running task 0.0 in stage 4.0 (TID 6)
15/02/01 22:53:47 INFO Executor: Running task 2.0 in stage 4.0 (TID 8)
15/02/01 22:53:47 INFO Executor: Running task 1.0 in stage 4.0 (TID 7)
15/02/01 22:53:47 INFO PythonRDD: Times: total = 30, boot = 11, init = 17, finish = 2
15/02/01 22:53:47 INFO Executor: Finished task 0.0 in stage 4.0 (TID 6). 689 bytes result sent to driver
15/02/01 22:53:47 INFO PythonRDD: Times: total = 30, boot = 18, init = 12, finish = 0
15/02/01 22:53:47 INFO PythonRDD: Times: total = 31, boot = 10, init = 20, finish = 1
15/02/01 22:53:47 INFO Executor: Finished task 1.0 in stage 4.0 (TID 7). 689 bytes result sent to driver
15/02/01 22:53:47 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 6) in 50 ms on localhost (1/3)
15/02/01 22:53:47 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 7) in 70 ms on localhost (2/3)
15/02/01 22:53:47 INFO Executor: Finished task 2.0 in stage 4.0 (TID 8). 689 bytes result sent to driver
15/02/01 22:53:47 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 8) in 90 ms on localhost (3/3)
15/02/01 22:53:47 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
15/02/01 22:53:47 INFO DAGScheduler: Stage 4 (count at <stdin>:1) finished in 0.072 s
15/02/01 22:53:47 INFO DAGScheduler: Job 3 finished: count at <stdin>:1, took 0.131217 s
6
>>> 
>>> myrdd2.take(6)
15/02/01 22:53:56 INFO SparkContext: Starting job: runJob at PythonRDD.scala:344
15/02/01 22:53:56 INFO DAGScheduler: Got job 4 (runJob at PythonRDD.scala:344) with 1 output partitions (allowLocal=true)
15/02/01 22:53:56 INFO DAGScheduler: Final stage: Stage 5(runJob at PythonRDD.scala:344)
15/02/01 22:53:56 INFO DAGScheduler: Parents of final stage: List()
15/02/01 22:53:56 INFO DAGScheduler: Missing parents: List()
15/02/01 22:53:56 INFO DAGScheduler: Submitting Stage 5 (PythonRDD[10] at RDD at PythonRDD.scala:43), which has no missing parents
15/02/01 22:53:56 INFO MemoryStore: ensureFreeSpace(3440) called with curMem=30086, maxMem=278302556
15/02/01 22:53:56 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 3.4 KB, free 265.4 MB)
15/02/01 22:53:56 INFO MemoryStore: ensureFreeSpace(2586) called with curMem=33526, maxMem=278302556
15/02/01 22:53:56 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.5 KB, free 265.4 MB)
15/02/01 22:53:56 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:43784 (size: 2.5 KB, free: 265.4 MB)
15/02/01 22:53:56 INFO BlockManagerMaster: Updated info of block broadcast_4_piece0
15/02/01 22:53:56 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:838
15/02/01 22:53:56 INFO DAGScheduler: Submitting 1 missing tasks from Stage 5 (PythonRDD[10] at RDD at PythonRDD.scala:43)
15/02/01 22:53:56 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
15/02/01 22:53:56 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 9, localhost, PROCESS_LOCAL, 1302 bytes)
15/02/01 22:53:56 INFO Executor: Running task 0.0 in stage 5.0 (TID 9)
15/02/01 22:53:56 INFO PythonRDD: Times: total = 45, boot = -8828, init = 8872, finish = 1
15/02/01 22:53:56 INFO Executor: Finished task 0.0 in stage 5.0 (TID 9). 722 bytes result sent to driver
15/02/01 22:53:56 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 9) in 72 ms on localhost (1/1)
15/02/01 22:53:56 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
15/02/01 22:53:56 INFO DAGScheduler: Stage 5 (runJob at PythonRDD.scala:344) finished in 0.074 s
15/02/01 22:53:56 INFO DAGScheduler: Job 4 finished: runJob at PythonRDD.scala:344, took 0.093691 s
15/02/01 22:53:56 INFO SparkContext: Starting job: runJob at PythonRDD.scala:344
15/02/01 22:53:56 INFO DAGScheduler: Got job 5 (runJob at PythonRDD.scala:344) with 2 output partitions (allowLocal=true)
15/02/01 22:53:56 INFO DAGScheduler: Final stage: Stage 6(runJob at PythonRDD.scala:344)
15/02/01 22:53:56 INFO DAGScheduler: Parents of final stage: List()
15/02/01 22:53:56 INFO DAGScheduler: Missing parents: List()
15/02/01 22:53:56 INFO DAGScheduler: Submitting Stage 6 (PythonRDD[11] at RDD at PythonRDD.scala:43), which has no missing parents
15/02/01 22:53:56 INFO MemoryStore: ensureFreeSpace(3440) called with curMem=36112, maxMem=278302556
15/02/01 22:53:56 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 3.4 KB, free 265.4 MB)
15/02/01 22:53:56 INFO MemoryStore: ensureFreeSpace(2586) called with curMem=39552, maxMem=278302556
15/02/01 22:53:56 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.5 KB, free 265.4 MB)
15/02/01 22:53:56 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:43784 (size: 2.5 KB, free: 265.4 MB)
15/02/01 22:53:56 INFO BlockManagerMaster: Updated info of block broadcast_5_piece0
15/02/01 22:53:56 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:838
15/02/01 22:53:56 INFO DAGScheduler: Submitting 2 missing tasks from Stage 6 (PythonRDD[11] at RDD at PythonRDD.scala:43)
15/02/01 22:53:56 INFO TaskSchedulerImpl: Adding task set 6.0 with 2 tasks
15/02/01 22:53:56 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 10, localhost, PROCESS_LOCAL, 1303 bytes)
15/02/01 22:53:56 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 11, localhost, PROCESS_LOCAL, 1305 bytes)
15/02/01 22:53:56 INFO Executor: Running task 0.0 in stage 6.0 (TID 10)
15/02/01 22:53:56 INFO Executor: Running task 1.0 in stage 6.0 (TID 11)
15/02/01 22:53:56 INFO PythonRDD: Times: total = 41, boot = -8990, init = 9031, finish = 0
15/02/01 22:53:56 INFO Executor: Finished task 0.0 in stage 6.0 (TID 10). 725 bytes result sent to driver
15/02/01 22:53:56 INFO PythonRDD: Times: total = 45, boot = -8985, init = 9029, finish = 1
15/02/01 22:53:56 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 10) in 69 ms on localhost (1/2)
15/02/01 22:53:56 INFO Executor: Finished task 1.0 in stage 6.0 (TID 11). 725 bytes result sent to driver
15/02/01 22:53:56 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 11) in 76 ms on localhost (2/2)
15/02/01 22:53:56 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
15/02/01 22:53:56 INFO DAGScheduler: Stage 6 (runJob at PythonRDD.scala:344) finished in 0.068 s
15/02/01 22:53:56 INFO DAGScheduler: Job 5 finished: runJob at PythonRDD.scala:344, took 0.111732 s
[('k1', 1), ('k2', 2), ('k1', -2), ('k3', 4), ('k2', -5), ('k1', 4)]
>>> 
>>> 
>>> reduced_rdd2 = myrdd2.reduceByKey(mylam)
>>> 
>>> reduced_rdd2.count()
15/02/01 22:54:16 INFO SparkContext: Starting job: count at <stdin>:1
15/02/01 22:54:16 INFO DAGScheduler: Registering RDD 13 (reduceByKey at <stdin>:1)
15/02/01 22:54:16 INFO DAGScheduler: Got job 6 (count at <stdin>:1) with 3 output partitions (allowLocal=false)
15/02/01 22:54:16 INFO DAGScheduler: Final stage: Stage 8(count at <stdin>:1)
15/02/01 22:54:16 INFO DAGScheduler: Parents of final stage: List(Stage 7)
15/02/01 22:54:16 INFO DAGScheduler: Missing parents: List(Stage 7)
15/02/01 22:54:16 INFO DAGScheduler: Submitting Stage 7 (PairwiseRDD[13] at reduceByKey at <stdin>:1), which has no missing parents
15/02/01 22:54:16 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=42138, maxMem=278302556
15/02/01 22:54:16 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 5.9 KB, free 265.4 MB)
15/02/01 22:54:16 INFO MemoryStore: ensureFreeSpace(4657) called with curMem=48178, maxMem=278302556
15/02/01 22:54:16 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 4.5 KB, free 265.4 MB)
15/02/01 22:54:16 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:43784 (size: 4.5 KB, free: 265.4 MB)
15/02/01 22:54:16 INFO BlockManagerMaster: Updated info of block broadcast_6_piece0
15/02/01 22:54:16 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:838
15/02/01 22:54:16 INFO DAGScheduler: Submitting 3 missing tasks from Stage 7 (PairwiseRDD[13] at reduceByKey at <stdin>:1)
15/02/01 22:54:16 INFO TaskSchedulerImpl: Adding task set 7.0 with 3 tasks
15/02/01 22:54:16 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 12, localhost, PROCESS_LOCAL, 1291 bytes)
15/02/01 22:54:16 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 13, localhost, PROCESS_LOCAL, 1292 bytes)
15/02/01 22:54:16 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 14, localhost, PROCESS_LOCAL, 1294 bytes)
15/02/01 22:54:16 INFO Executor: Running task 0.0 in stage 7.0 (TID 12)
15/02/01 22:54:16 INFO Executor: Running task 2.0 in stage 7.0 (TID 14)
15/02/01 22:54:16 INFO Executor: Running task 1.0 in stage 7.0 (TID 13)
15/02/01 22:54:16 INFO PythonRDD: Times: total = 49, boot = -19345, init = 19393, finish = 1
15/02/01 22:54:16 INFO PythonRDD: Times: total = 47, boot = -19329, init = 19375, finish = 1
15/02/01 22:54:16 INFO PythonRDD: Times: total = 41, boot = -19477, init = 19518, finish = 0
15/02/01 22:54:16 INFO Executor: Finished task 2.0 in stage 7.0 (TID 14). 901 bytes result sent to driver
15/02/01 22:54:16 INFO Executor: Finished task 0.0 in stage 7.0 (TID 12). 901 bytes result sent to driver
15/02/01 22:54:16 INFO Executor: Finished task 1.0 in stage 7.0 (TID 13). 901 bytes result sent to driver
15/02/01 22:54:16 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 14) in 140 ms on localhost (1/3)
15/02/01 22:54:16 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 12) in 166 ms on localhost (2/3)
15/02/01 22:54:16 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 13) in 158 ms on localhost (3/3)
15/02/01 22:54:16 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
15/02/01 22:54:16 INFO DAGScheduler: Stage 7 (reduceByKey at <stdin>:1) finished in 0.159 s
15/02/01 22:54:16 INFO DAGScheduler: looking for newly runnable stages
15/02/01 22:54:16 INFO DAGScheduler: running: Set()
15/02/01 22:54:16 INFO DAGScheduler: waiting: Set(Stage 8)
15/02/01 22:54:16 INFO DAGScheduler: failed: Set()
15/02/01 22:54:16 INFO DAGScheduler: Missing parents for Stage 8: List()
15/02/01 22:54:16 INFO DAGScheduler: Submitting Stage 8 (PythonRDD[16] at count at <stdin>:1), which is now runnable
15/02/01 22:54:16 INFO MemoryStore: ensureFreeSpace(5736) called with curMem=52835, maxMem=278302556
15/02/01 22:54:16 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 5.6 KB, free 265.4 MB)
15/02/01 22:54:16 INFO MemoryStore: ensureFreeSpace(4238) called with curMem=58571, maxMem=278302556
15/02/01 22:54:16 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 4.1 KB, free 265.4 MB)
15/02/01 22:54:16 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:43784 (size: 4.1 KB, free: 265.4 MB)
15/02/01 22:54:16 INFO BlockManagerMaster: Updated info of block broadcast_7_piece0
15/02/01 22:54:16 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:838
15/02/01 22:54:16 INFO DAGScheduler: Submitting 3 missing tasks from Stage 8 (PythonRDD[16] at count at <stdin>:1)
15/02/01 22:54:16 INFO TaskSchedulerImpl: Adding task set 8.0 with 3 tasks
15/02/01 22:54:16 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 15, localhost, PROCESS_LOCAL, 1056 bytes)
15/02/01 22:54:16 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 16, localhost, PROCESS_LOCAL, 1056 bytes)
15/02/01 22:54:16 INFO TaskSetManager: Starting task 2.0 in stage 8.0 (TID 17, localhost, PROCESS_LOCAL, 1056 bytes)
15/02/01 22:54:16 INFO Executor: Running task 0.0 in stage 8.0 (TID 15)
15/02/01 22:54:16 INFO Executor: Running task 1.0 in stage 8.0 (TID 16)
15/02/01 22:54:16 INFO Executor: Running task 2.0 in stage 8.0 (TID 17)
15/02/01 22:54:16 INFO ShuffleBlockFetcherIterator: Getting 3 non-empty blocks out of 3 blocks
15/02/01 22:54:16 INFO ShuffleBlockFetcherIterator: Getting 3 non-empty blocks out of 3 blocks
15/02/01 22:54:16 INFO ShuffleBlockFetcherIterator: Getting 3 non-empty blocks out of 3 blocks
15/02/01 22:54:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/02/01 22:54:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/02/01 22:54:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/02/01 22:54:16 INFO PythonRDD: Times: total = 93, boot = -102, init = 195, finish = 0
15/02/01 22:54:16 INFO PythonRDD: Times: total = 92, boot = -110, init = 201, finish = 1
15/02/01 22:54:16 INFO PythonRDD: Times: total = 92, boot = -133, init = 225, finish = 0
15/02/01 22:54:16 INFO Executor: Finished task 2.0 in stage 8.0 (TID 17). 902 bytes result sent to driver
15/02/01 22:54:16 INFO Executor: Finished task 0.0 in stage 8.0 (TID 15). 902 bytes result sent to driver
15/02/01 22:54:16 INFO Executor: Finished task 1.0 in stage 8.0 (TID 16). 902 bytes result sent to driver
15/02/01 22:54:16 INFO TaskSetManager: Finished task 2.0 in stage 8.0 (TID 17) in 146 ms on localhost (1/3)
15/02/01 22:54:16 INFO TaskSetManager: Finished task 1.0 in stage 8.0 (TID 16) in 151 ms on localhost (2/3)
15/02/01 22:54:16 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 15) in 153 ms on localhost (3/3)
15/02/01 22:54:16 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
15/02/01 22:54:16 INFO DAGScheduler: Stage 8 (count at <stdin>:1) finished in 0.162 s
15/02/01 22:54:16 INFO DAGScheduler: Job 6 finished: count at <stdin>:1, took 0.378387 s
3
>>> 
>>> 
>>> reduced_rdd2.take(reduced_rdd2.count())
15/02/01 22:54:23 INFO SparkContext: Starting job: count at <stdin>:1
15/02/01 22:54:23 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 162 bytes
15/02/01 22:54:23 INFO DAGScheduler: Got job 7 (count at <stdin>:1) with 3 output partitions (allowLocal=false)
15/02/01 22:54:23 INFO DAGScheduler: Final stage: Stage 10(count at <stdin>:1)
15/02/01 22:54:23 INFO DAGScheduler: Parents of final stage: List(Stage 9)
15/02/01 22:54:23 INFO DAGScheduler: Missing parents: List()
15/02/01 22:54:23 INFO DAGScheduler: Submitting Stage 10 (PythonRDD[17] at count at <stdin>:1), which has no missing parents
15/02/01 22:54:23 INFO MemoryStore: ensureFreeSpace(5736) called with curMem=62809, maxMem=278302556
15/02/01 22:54:23 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 5.6 KB, free 265.3 MB)
15/02/01 22:54:23 INFO MemoryStore: ensureFreeSpace(4238) called with curMem=68545, maxMem=278302556
15/02/01 22:54:23 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 4.1 KB, free 265.3 MB)
15/02/01 22:54:23 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:43784 (size: 4.1 KB, free: 265.4 MB)
15/02/01 22:54:23 INFO BlockManagerMaster: Updated info of block broadcast_8_piece0
15/02/01 22:54:23 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:838
15/02/01 22:54:23 INFO DAGScheduler: Submitting 3 missing tasks from Stage 10 (PythonRDD[17] at count at <stdin>:1)
15/02/01 22:54:23 INFO TaskSchedulerImpl: Adding task set 10.0 with 3 tasks
15/02/01 22:54:23 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 18, localhost, PROCESS_LOCAL, 1056 bytes)
15/02/01 22:54:23 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 19, localhost, PROCESS_LOCAL, 1056 bytes)
15/02/01 22:54:23 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 20, localhost, PROCESS_LOCAL, 1056 bytes)
15/02/01 22:54:23 INFO Executor: Running task 0.0 in stage 10.0 (TID 18)
15/02/01 22:54:23 INFO Executor: Running task 2.0 in stage 10.0 (TID 20)
15/02/01 22:54:23 INFO Executor: Running task 1.0 in stage 10.0 (TID 19)
15/02/01 22:54:23 INFO ShuffleBlockFetcherIterator: Getting 3 non-empty blocks out of 3 blocks
15/02/01 22:54:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/02/01 22:54:23 INFO ShuffleBlockFetcherIterator: Getting 3 non-empty blocks out of 3 blocks
15/02/01 22:54:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/02/01 22:54:23 INFO ShuffleBlockFetcherIterator: Getting 3 non-empty blocks out of 3 blocks
15/02/01 22:54:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/02/01 22:54:23 INFO PythonRDD: Times: total = 50, boot = -7126, init = 7174, finish = 2
15/02/01 22:54:23 INFO Executor: Finished task 0.0 in stage 10.0 (TID 18). 902 bytes result sent to driver
15/02/01 22:54:23 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 18) in 72 ms on localhost (1/3)
15/02/01 22:54:23 INFO PythonRDD: Times: total = 45, boot = -7146, init = 7191, finish = 0
15/02/01 22:54:23 INFO PythonRDD: Times: total = 54, boot = -7145, init = 7199, finish = 0
15/02/01 22:54:23 INFO Executor: Finished task 2.0 in stage 10.0 (TID 20). 902 bytes result sent to driver
15/02/01 22:54:23 INFO Executor: Finished task 1.0 in stage 10.0 (TID 19). 902 bytes result sent to driver
15/02/01 22:54:23 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 20) in 105 ms on localhost (2/3)
15/02/01 22:54:23 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 19) in 113 ms on localhost (3/3)
15/02/01 22:54:23 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
15/02/01 22:54:23 INFO DAGScheduler: Stage 10 (count at <stdin>:1) finished in 0.124 s
15/02/01 22:54:23 INFO DAGScheduler: Job 7 finished: count at <stdin>:1, took 0.230520 s
15/02/01 22:54:23 INFO SparkContext: Starting job: runJob at PythonRDD.scala:344
15/02/01 22:54:23 INFO DAGScheduler: Got job 8 (runJob at PythonRDD.scala:344) with 1 output partitions (allowLocal=true)
15/02/01 22:54:23 INFO DAGScheduler: Final stage: Stage 12(runJob at PythonRDD.scala:344)
15/02/01 22:54:23 INFO DAGScheduler: Parents of final stage: List(Stage 11)
15/02/01 22:54:23 INFO DAGScheduler: Missing parents: List()
15/02/01 22:54:23 INFO DAGScheduler: Submitting Stage 12 (PythonRDD[19] at RDD at PythonRDD.scala:43), which has no missing parents
15/02/01 22:54:23 INFO MemoryStore: ensureFreeSpace(5136) called with curMem=72783, maxMem=278302556
15/02/01 22:54:23 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 5.0 KB, free 265.3 MB)
15/02/01 22:54:23 INFO MemoryStore: ensureFreeSpace(3816) called with curMem=77919, maxMem=278302556
15/02/01 22:54:23 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 3.7 KB, free 265.3 MB)
15/02/01 22:54:23 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:43784 (size: 3.7 KB, free: 265.4 MB)
15/02/01 22:54:23 INFO BlockManagerMaster: Updated info of block broadcast_9_piece0
15/02/01 22:54:23 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:838
15/02/01 22:54:23 INFO DAGScheduler: Submitting 1 missing tasks from Stage 12 (PythonRDD[19] at RDD at PythonRDD.scala:43)
15/02/01 22:54:23 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks
15/02/01 22:54:23 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 21, localhost, PROCESS_LOCAL, 1056 bytes)
15/02/01 22:54:23 INFO Executor: Running task 0.0 in stage 12.0 (TID 21)
15/02/01 22:54:23 INFO ShuffleBlockFetcherIterator: Getting 3 non-empty blocks out of 3 blocks
15/02/01 22:54:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/02/01 22:54:23 INFO PythonRDD: Times: total = 43, boot = -89, init = 132, finish = 0
15/02/01 22:54:23 INFO Executor: Finished task 0.0 in stage 12.0 (TID 21). 914 bytes result sent to driver
15/02/01 22:54:23 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 21) in 50 ms on localhost (1/1)
15/02/01 22:54:23 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
15/02/01 22:54:23 INFO DAGScheduler: Stage 12 (runJob at PythonRDD.scala:344) finished in 0.055 s
15/02/01 22:54:23 INFO DAGScheduler: Job 8 finished: runJob at PythonRDD.scala:344, took 0.079097 s
15/02/01 22:54:23 INFO SparkContext: Starting job: runJob at PythonRDD.scala:344
15/02/01 22:54:23 INFO DAGScheduler: Got job 9 (runJob at PythonRDD.scala:344) with 2 output partitions (allowLocal=true)
15/02/01 22:54:23 INFO DAGScheduler: Final stage: Stage 14(runJob at PythonRDD.scala:344)
15/02/01 22:54:23 INFO DAGScheduler: Parents of final stage: List(Stage 13)
15/02/01 22:54:23 INFO DAGScheduler: Missing parents: List()
15/02/01 22:54:23 INFO DAGScheduler: Submitting Stage 14 (PythonRDD[20] at RDD at PythonRDD.scala:43), which has no missing parents
15/02/01 22:54:23 INFO MemoryStore: ensureFreeSpace(5136) called with curMem=81735, maxMem=278302556
15/02/01 22:54:23 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 5.0 KB, free 265.3 MB)
15/02/01 22:54:24 INFO MemoryStore: ensureFreeSpace(3816) called with curMem=86871, maxMem=278302556
15/02/01 22:54:24 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.7 KB, free 265.3 MB)
15/02/01 22:54:24 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:43784 (size: 3.7 KB, free: 265.4 MB)
15/02/01 22:54:24 INFO BlockManagerMaster: Updated info of block broadcast_10_piece0
15/02/01 22:54:24 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:838
15/02/01 22:54:24 INFO DAGScheduler: Submitting 2 missing tasks from Stage 14 (PythonRDD[20] at RDD at PythonRDD.scala:43)
15/02/01 22:54:24 INFO TaskSchedulerImpl: Adding task set 14.0 with 2 tasks
15/02/01 22:54:24 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 22, localhost, PROCESS_LOCAL, 1056 bytes)
15/02/01 22:54:24 INFO TaskSetManager: Starting task 1.0 in stage 14.0 (TID 23, localhost, PROCESS_LOCAL, 1056 bytes)
15/02/01 22:54:24 INFO Executor: Running task 0.0 in stage 14.0 (TID 22)
15/02/01 22:54:24 INFO Executor: Running task 1.0 in stage 14.0 (TID 23)
15/02/01 22:54:24 INFO ShuffleBlockFetcherIterator: Getting 3 non-empty blocks out of 3 blocks
15/02/01 22:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/02/01 22:54:24 INFO ShuffleBlockFetcherIterator: Getting 3 non-empty blocks out of 3 blocks
15/02/01 22:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/02/01 22:54:24 INFO PythonRDD: Times: total = 56, boot = -166, init = 221, finish = 1
15/02/01 22:54:24 INFO Executor: Finished task 0.0 in stage 14.0 (TID 22). 911 bytes result sent to driver
15/02/01 22:54:24 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 22) in 69 ms on localhost (1/2)
15/02/01 22:54:24 INFO PythonRDD: Times: total = 41, boot = -201, init = 242, finish = 0
15/02/01 22:54:24 INFO Executor: Finished task 1.0 in stage 14.0 (TID 23). 911 bytes result sent to driver
15/02/01 22:54:24 INFO TaskSetManager: Finished task 1.0 in stage 14.0 (TID 23) in 78 ms on localhost (2/2)
15/02/01 22:54:24 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
15/02/01 22:54:24 INFO DAGScheduler: Stage 14 (runJob at PythonRDD.scala:344) finished in 0.079 s
15/02/01 22:54:24 INFO DAGScheduler: Job 9 finished: runJob at PythonRDD.scala:344, took 0.113889 s
[('k2', -3), ('k1', 3), ('k3', 4)]
>>> 
>>> 
>>> mylist3 = [ ['k1', 1], ['k2', 2], ['k1', -2], ['k3', 4], ['k2', -5], ['k1', 4] ]
>>> 
>>> myrdd3 = sc.parallelize(mylist3)
>>> 
>>> myrdd3.count()
15/02/01 22:54:58 INFO SparkContext: Starting job: count at <stdin>:1
15/02/01 22:54:58 INFO DAGScheduler: Got job 10 (count at <stdin>:1) with 3 output partitions (allowLocal=false)
15/02/01 22:54:58 INFO DAGScheduler: Final stage: Stage 15(count at <stdin>:1)
15/02/01 22:54:58 INFO DAGScheduler: Parents of final stage: List()
15/02/01 22:54:58 INFO DAGScheduler: Missing parents: List()
15/02/01 22:54:58 INFO DAGScheduler: Submitting Stage 15 (PythonRDD[22] at count at <stdin>:1), which has no missing parents
15/02/01 22:54:58 INFO MemoryStore: ensureFreeSpace(4192) called with curMem=90687, maxMem=278302556
15/02/01 22:54:58 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 4.1 KB, free 265.3 MB)
15/02/01 22:54:58 INFO MemoryStore: ensureFreeSpace(3148) called with curMem=94879, maxMem=278302556
15/02/01 22:54:58 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 3.1 KB, free 265.3 MB)
15/02/01 22:54:58 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on localhost:43784 (size: 3.1 KB, free: 265.4 MB)
15/02/01 22:54:58 INFO BlockManagerMaster: Updated info of block broadcast_11_piece0
15/02/01 22:54:58 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:838
15/02/01 22:54:58 INFO DAGScheduler: Submitting 3 missing tasks from Stage 15 (PythonRDD[22] at count at <stdin>:1)
15/02/01 22:54:58 INFO TaskSchedulerImpl: Adding task set 15.0 with 3 tasks
15/02/01 22:54:58 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 24, localhost, PROCESS_LOCAL, 1306 bytes)
15/02/01 22:54:58 INFO TaskSetManager: Starting task 1.0 in stage 15.0 (TID 25, localhost, PROCESS_LOCAL, 1309 bytes)
15/02/01 22:54:58 INFO TaskSetManager: Starting task 2.0 in stage 15.0 (TID 26, localhost, PROCESS_LOCAL, 1309 bytes)
15/02/01 22:54:58 INFO Executor: Running task 0.0 in stage 15.0 (TID 24)
15/02/01 22:54:58 INFO Executor: Running task 1.0 in stage 15.0 (TID 25)
15/02/01 22:54:58 INFO Executor: Running task 2.0 in stage 15.0 (TID 26)
15/02/01 22:54:58 INFO PythonRDD: Times: total = 42, boot = -34777, init = 34819, finish = 0
15/02/01 22:54:58 INFO Executor: Finished task 1.0 in stage 15.0 (TID 25). 689 bytes result sent to driver
15/02/01 22:54:58 INFO TaskSetManager: Finished task 1.0 in stage 15.0 (TID 25) in 66 ms on localhost (1/3)
15/02/01 22:54:58 INFO PythonRDD: Times: total = 51, boot = -34657, init = 34708, finish = 0
15/02/01 22:54:58 INFO Executor: Finished task 0.0 in stage 15.0 (TID 24). 689 bytes result sent to driver
15/02/01 22:54:58 INFO PythonRDD: Times: total = 46, boot = -34659, init = 34705, finish = 0
15/02/01 22:54:58 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 24) in 87 ms on localhost (2/3)
15/02/01 22:54:58 INFO Executor: Finished task 2.0 in stage 15.0 (TID 26). 689 bytes result sent to driver
15/02/01 22:54:58 INFO TaskSetManager: Finished task 2.0 in stage 15.0 (TID 26) in 95 ms on localhost (3/3)
15/02/01 22:54:58 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
15/02/01 22:54:58 INFO DAGScheduler: Stage 15 (count at <stdin>:1) finished in 0.101 s
15/02/01 22:54:58 INFO DAGScheduler: Job 10 finished: count at <stdin>:1, took 0.128758 s
6
>>> 
>>> 
>>> myrdd3.take(6)
15/02/01 22:55:07 INFO SparkContext: Starting job: runJob at PythonRDD.scala:344
15/02/01 22:55:07 INFO DAGScheduler: Got job 11 (runJob at PythonRDD.scala:344) with 1 output partitions (allowLocal=true)
15/02/01 22:55:07 INFO DAGScheduler: Final stage: Stage 16(runJob at PythonRDD.scala:344)
15/02/01 22:55:07 INFO DAGScheduler: Parents of final stage: List()
15/02/01 22:55:07 INFO DAGScheduler: Missing parents: List()
15/02/01 22:55:07 INFO DAGScheduler: Submitting Stage 16 (PythonRDD[23] at RDD at PythonRDD.scala:43), which has no missing parents
15/02/01 22:55:07 INFO MemoryStore: ensureFreeSpace(3440) called with curMem=98027, maxMem=278302556
15/02/01 22:55:07 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 3.4 KB, free 265.3 MB)
15/02/01 22:55:07 INFO MemoryStore: ensureFreeSpace(2587) called with curMem=101467, maxMem=278302556
15/02/01 22:55:07 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 2.5 KB, free 265.3 MB)
15/02/01 22:55:07 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on localhost:43784 (size: 2.5 KB, free: 265.4 MB)
15/02/01 22:55:07 INFO BlockManagerMaster: Updated info of block broadcast_12_piece0
15/02/01 22:55:07 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:838
15/02/01 22:55:07 INFO DAGScheduler: Submitting 1 missing tasks from Stage 16 (PythonRDD[23] at RDD at PythonRDD.scala:43)
15/02/01 22:55:07 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks
15/02/01 22:55:07 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 27, localhost, PROCESS_LOCAL, 1306 bytes)
15/02/01 22:55:07 INFO Executor: Running task 0.0 in stage 16.0 (TID 27)
15/02/01 22:55:07 INFO BlockManager: Removing broadcast 0
15/02/01 22:55:07 INFO BlockManager: Removing block broadcast_0
15/02/01 22:55:07 INFO MemoryStore: Block broadcast_0 of size 3440 dropped from memory (free 278201942)
15/02/01 22:55:07 INFO BlockManager: Removing block broadcast_0_piece0
15/02/01 22:55:07 INFO MemoryStore: Block broadcast_0_piece0 of size 2587 dropped from memory (free 278204529)
15/02/01 22:55:07 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:43784 in memory (size: 2.5 KB, free: 265.4 MB)
15/02/01 22:55:07 INFO BlockManagerMaster: Updated info of block broadcast_0_piece0
15/02/01 22:55:07 INFO ContextCleaner: Cleaned broadcast 0
15/02/01 22:55:07 INFO PythonRDD: Times: total = 40, boot = -8369, init = 8409, finish = 0
15/02/01 22:55:07 INFO Executor: Finished task 0.0 in stage 16.0 (TID 27). 730 bytes result sent to driver
15/02/01 22:55:07 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 27) in 70 ms on localhost (1/1)
15/02/01 22:55:07 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
15/02/01 22:55:07 INFO DAGScheduler: Stage 16 (runJob at PythonRDD.scala:344) finished in 0.070 s
15/02/01 22:55:07 INFO DAGScheduler: Job 11 finished: runJob at PythonRDD.scala:344, took 0.163758 s
15/02/01 22:55:07 INFO BlockManager: Removing broadcast 11
15/02/01 22:55:07 INFO BlockManager: Removing block broadcast_11
15/02/01 22:55:07 INFO MemoryStore: Block broadcast_11 of size 4192 dropped from memory (free 278208721)
15/02/01 22:55:07 INFO BlockManager: Removing block broadcast_11_piece0
15/02/01 22:55:07 INFO MemoryStore: Block broadcast_11_piece0 of size 3148 dropped from memory (free 278211869)
15/02/01 22:55:07 INFO BlockManagerInfo: Removed broadcast_11_piece0 on localhost:43784 in memory (size: 3.1 KB, free: 265.4 MB)
15/02/01 22:55:07 INFO BlockManagerMaster: Updated info of block broadcast_11_piece0
15/02/01 22:55:07 INFO ContextCleaner: Cleaned broadcast 11
15/02/01 22:55:07 INFO BlockManager: Removing broadcast 10
15/02/01 22:55:07 INFO BlockManager: Removing block broadcast_10
15/02/01 22:55:07 INFO MemoryStore: Block broadcast_10 of size 5136 dropped from memory (free 278217005)
15/02/01 22:55:07 INFO BlockManager: Removing block broadcast_10_piece0
15/02/01 22:55:07 INFO MemoryStore: Block broadcast_10_piece0 of size 3816 dropped from memory (free 278220821)
15/02/01 22:55:07 INFO BlockManagerInfo: Removed broadcast_10_piece0 on localhost:43784 in memory (size: 3.7 KB, free: 265.4 MB)
15/02/01 22:55:07 INFO BlockManagerMaster: Updated info of block broadcast_10_piece0
15/02/01 22:55:07 INFO ContextCleaner: Cleaned broadcast 10
15/02/01 22:55:07 INFO BlockManager: Removing broadcast 9
15/02/01 22:55:07 INFO BlockManager: Removing block broadcast_9
15/02/01 22:55:07 INFO MemoryStore: Block broadcast_9 of size 5136 dropped from memory (free 278225957)
15/02/01 22:55:07 INFO BlockManager: Removing block broadcast_9_piece0
15/02/01 22:55:07 INFO MemoryStore: Block broadcast_9_piece0 of size 3816 dropped from memory (free 278229773)
15/02/01 22:55:07 INFO BlockManagerInfo: Removed broadcast_9_piece0 on localhost:43784 in memory (size: 3.7 KB, free: 265.4 MB)
15/02/01 22:55:07 INFO BlockManagerMaster: Updated info of block broadcast_9_piece0
15/02/01 22:55:07 INFO ContextCleaner: Cleaned broadcast 9
15/02/01 22:55:07 INFO BlockManager: Removing broadcast 8
15/02/01 22:55:07 INFO BlockManager: Removing block broadcast_8
15/02/01 22:55:07 INFO MemoryStore: Block broadcast_8 of size 5736 dropped from memory (free 278235509)
15/02/01 22:55:07 INFO BlockManager: Removing block broadcast_8_piece0
15/02/01 22:55:07 INFO MemoryStore: Block broadcast_8_piece0 of size 4238 dropped from memory (free 278239747)
15/02/01 22:55:07 INFO BlockManagerInfo: Removed broadcast_8_piece0 on localhost:43784 in memory (size: 4.1 KB, free: 265.4 MB)
15/02/01 22:55:07 INFO BlockManagerMaster: Updated info of block broadcast_8_piece0
15/02/01 22:55:07 INFO ContextCleaner: Cleaned broadcast 8
15/02/01 22:55:07 INFO BlockManager: Removing broadcast 7
15/02/01 22:55:07 INFO BlockManager: Removing block broadcast_7
15/02/01 22:55:07 INFO MemoryStore: Block broadcast_7 of size 5736 dropped from memory (free 278245483)
15/02/01 22:55:07 INFO BlockManager: Removing block broadcast_7_piece0
15/02/01 22:55:07 INFO MemoryStore: Block broadcast_7_piece0 of size 4238 dropped from memory (free 278249721)
15/02/01 22:55:07 INFO BlockManagerInfo: Removed broadcast_7_piece0 on localhost:43784 in memory (size: 4.1 KB, free: 265.4 MB)
15/02/01 22:55:07 INFO BlockManagerMaster: Updated info of block broadcast_7_piece0
15/02/01 22:55:07 INFO ContextCleaner: Cleaned broadcast 7
15/02/01 22:55:07 INFO BlockManager: Removing broadcast 6
15/02/01 22:55:07 INFO BlockManager: Removing block broadcast_6_piece0
15/02/01 22:55:07 INFO MemoryStore: Block broadcast_6_piece0 of size 4657 dropped from memory (free 278254378)
15/02/01 22:55:07 INFO BlockManagerInfo: Removed broadcast_6_piece0 on localhost:43784 in memory (size: 4.5 KB, free: 265.4 MB)
15/02/01 22:55:07 INFO BlockManagerMaster: Updated info of block broadcast_6_piece0
15/02/01 22:55:07 INFO BlockManager: Removing block broadcast_6
15/02/01 22:55:07 INFO MemoryStore: Block broadcast_6 of size 6040 dropped from memory (free 278260418)
15/02/01 22:55:07 INFO ContextCleaner: Cleaned broadcast 6
15/02/01 22:55:07 INFO BlockManager: Removing broadcast 5
15/02/01 22:55:07 INFO BlockManager: Removing block broadcast_5_piece0
15/02/01 22:55:07 INFO MemoryStore: Block broadcast_5_piece0 of size 2586 dropped from memory (free 278263004)
15/02/01 22:55:07 INFO BlockManagerInfo: Removed broadcast_5_piece0 on localhost:43784 in memory (size: 2.5 KB, free: 265.4 MB)
15/02/01 22:55:07 INFO BlockManagerMaster: Updated info of block broadcast_5_piece0
15/02/01 22:55:07 INFO BlockManager: Removing block broadcast_5
15/02/01 22:55:07 INFO MemoryStore: Block broadcast_5 of size 3440 dropped from memory (free 278266444)
15/02/01 22:55:07 INFO ContextCleaner: Cleaned broadcast 5
15/02/01 22:55:07 INFO BlockManager: Removing broadcast 4
15/02/01 22:55:07 INFO BlockManager: Removing block broadcast_4
15/02/01 22:55:07 INFO MemoryStore: Block broadcast_4 of size 3440 dropped from memory (free 278269884)
15/02/01 22:55:07 INFO BlockManager: Removing block broadcast_4_piece0
15/02/01 22:55:07 INFO MemoryStore: Block broadcast_4_piece0 of size 2586 dropped from memory (free 278272470)
15/02/01 22:55:07 INFO BlockManagerInfo: Removed broadcast_4_piece0 on localhost:43784 in memory (size: 2.5 KB, free: 265.4 MB)
15/02/01 22:55:07 INFO BlockManagerMaster: Updated info of block broadcast_4_piece0
15/02/01 22:55:07 INFO ContextCleaner: Cleaned broadcast 4
15/02/01 22:55:07 INFO SparkContext: Starting job: runJob at PythonRDD.scala:344
15/02/01 22:55:07 INFO BlockManager: Removing broadcast 3
15/02/01 22:55:07 INFO BlockManager: Removing block broadcast_3_piece0
15/02/01 22:55:07 INFO MemoryStore: Block broadcast_3_piece0 of size 3147 dropped from memory (free 278275617)
15/02/01 22:55:07 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:43784 in memory (size: 3.1 KB, free: 265.4 MB)
15/02/01 22:55:07 INFO BlockManagerMaster: Updated info of block broadcast_3_piece0
15/02/01 22:55:07 INFO BlockManager: Removing block broadcast_3
15/02/01 22:55:07 INFO MemoryStore: Block broadcast_3 of size 4192 dropped from memory (free 278279809)
15/02/01 22:55:07 INFO ContextCleaner: Cleaned broadcast 3
15/02/01 22:55:07 INFO DAGScheduler: Got job 12 (runJob at PythonRDD.scala:344) with 2 output partitions (allowLocal=true)
15/02/01 22:55:07 INFO DAGScheduler: Final stage: Stage 17(runJob at PythonRDD.scala:344)
15/02/01 22:55:07 INFO DAGScheduler: Parents of final stage: List()
15/02/01 22:55:07 INFO BlockManager: Removing broadcast 2
15/02/01 22:55:07 INFO BlockManager: Removing block broadcast_2
15/02/01 22:55:07 INFO MemoryStore: Block broadcast_2 of size 6040 dropped from memory (free 278285849)
15/02/01 22:55:07 INFO BlockManager: Removing block broadcast_2_piece0
15/02/01 22:55:07 INFO MemoryStore: Block broadcast_2_piece0 of size 4653 dropped from memory (free 278290502)
15/02/01 22:55:07 INFO BlockManagerInfo: Removed broadcast_2_piece0 on localhost:43784 in memory (size: 4.5 KB, free: 265.4 MB)
15/02/01 22:55:07 INFO BlockManagerMaster: Updated info of block broadcast_2_piece0
15/02/01 22:55:07 INFO DAGScheduler: Missing parents: List()
15/02/01 22:55:07 INFO ContextCleaner: Cleaned broadcast 2
15/02/01 22:55:07 INFO DAGScheduler: Submitting Stage 17 (PythonRDD[24] at RDD at PythonRDD.scala:43), which has no missing parents
15/02/01 22:55:07 INFO BlockManager: Removing broadcast 1
15/02/01 22:55:07 INFO BlockManager: Removing block broadcast_1_piece0
15/02/01 22:55:07 INFO MemoryStore: Block broadcast_1_piece0 of size 2587 dropped from memory (free 278293089)
15/02/01 22:55:07 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:43784 in memory (size: 2.5 KB, free: 265.4 MB)
15/02/01 22:55:07 INFO BlockManagerMaster: Updated info of block broadcast_1_piece0
15/02/01 22:55:07 INFO BlockManager: Removing block broadcast_1
15/02/01 22:55:07 INFO MemoryStore: Block broadcast_1 of size 3440 dropped from memory (free 278296529)
15/02/01 22:55:07 INFO MemoryStore: ensureFreeSpace(3440) called with curMem=6027, maxMem=278302556
15/02/01 22:55:07 INFO ContextCleaner: Cleaned broadcast 1
15/02/01 22:55:07 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 3.4 KB, free 265.4 MB)
15/02/01 22:55:07 INFO MemoryStore: ensureFreeSpace(2587) called with curMem=9467, maxMem=278302556
15/02/01 22:55:07 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 2.5 KB, free 265.4 MB)
15/02/01 22:55:07 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on localhost:43784 (size: 2.5 KB, free: 265.4 MB)
15/02/01 22:55:07 INFO BlockManagerMaster: Updated info of block broadcast_13_piece0
15/02/01 22:55:07 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:838
15/02/01 22:55:07 INFO DAGScheduler: Submitting 2 missing tasks from Stage 17 (PythonRDD[24] at RDD at PythonRDD.scala:43)
15/02/01 22:55:07 INFO TaskSchedulerImpl: Adding task set 17.0 with 2 tasks
15/02/01 22:55:07 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 28, localhost, PROCESS_LOCAL, 1309 bytes)
15/02/01 22:55:07 INFO TaskSetManager: Starting task 1.0 in stage 17.0 (TID 29, localhost, PROCESS_LOCAL, 1309 bytes)
15/02/01 22:55:07 INFO Executor: Running task 0.0 in stage 17.0 (TID 28)
15/02/01 22:55:07 INFO Executor: Running task 1.0 in stage 17.0 (TID 29)
15/02/01 22:55:07 INFO PythonRDD: Times: total = 43, boot = -8559, init = 8602, finish = 0
15/02/01 22:55:07 INFO Executor: Finished task 1.0 in stage 17.0 (TID 29). 733 bytes result sent to driver
15/02/01 22:55:07 INFO TaskSetManager: Finished task 1.0 in stage 17.0 (TID 29) in 67 ms on localhost (1/2)
15/02/01 22:55:07 INFO PythonRDD: Times: total = 48, boot = -8553, init = 8601, finish = 0
15/02/01 22:55:07 INFO Executor: Finished task 0.0 in stage 17.0 (TID 28). 733 bytes result sent to driver
15/02/01 22:55:07 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 28) in 97 ms on localhost (2/2)
15/02/01 22:55:07 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
15/02/01 22:55:07 INFO DAGScheduler: Stage 17 (runJob at PythonRDD.scala:344) finished in 0.099 s
15/02/01 22:55:07 INFO DAGScheduler: Job 12 finished: runJob at PythonRDD.scala:344, took 0.127290 s
[['k1', 1], ['k2', 2], ['k1', -2], ['k3', 4], ['k2', -5], ['k1', 4]]
>>> 
>>> 
>>> reduced_rdd3 = myrdd3.reduceByKey(mylam)
>>> 
>>> 
>>> reduced_rdd3.count()
15/02/01 22:55:28 INFO SparkContext: Starting job: count at <stdin>:1
15/02/01 22:55:28 INFO DAGScheduler: Registering RDD 26 (reduceByKey at <stdin>:1)
15/02/01 22:55:28 INFO DAGScheduler: Got job 13 (count at <stdin>:1) with 3 output partitions (allowLocal=false)
15/02/01 22:55:28 INFO DAGScheduler: Final stage: Stage 19(count at <stdin>:1)
15/02/01 22:55:28 INFO DAGScheduler: Parents of final stage: List(Stage 18)
15/02/01 22:55:28 INFO DAGScheduler: Missing parents: List(Stage 18)
15/02/01 22:55:28 INFO DAGScheduler: Submitting Stage 18 (PairwiseRDD[26] at reduceByKey at <stdin>:1), which has no missing parents
15/02/01 22:55:28 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=12054, maxMem=278302556
15/02/01 22:55:28 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 5.9 KB, free 265.4 MB)
15/02/01 22:55:28 INFO MemoryStore: ensureFreeSpace(4657) called with curMem=18094, maxMem=278302556
15/02/01 22:55:28 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 4.5 KB, free 265.4 MB)
15/02/01 22:55:28 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on localhost:43784 (size: 4.5 KB, free: 265.4 MB)
15/02/01 22:55:28 INFO BlockManagerMaster: Updated info of block broadcast_14_piece0
15/02/01 22:55:28 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:838
15/02/01 22:55:28 INFO DAGScheduler: Submitting 3 missing tasks from Stage 18 (PairwiseRDD[26] at reduceByKey at <stdin>:1)
15/02/01 22:55:28 INFO TaskSchedulerImpl: Adding task set 18.0 with 3 tasks
15/02/01 22:55:28 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 30, localhost, PROCESS_LOCAL, 1295 bytes)
15/02/01 22:55:28 INFO TaskSetManager: Starting task 1.0 in stage 18.0 (TID 31, localhost, PROCESS_LOCAL, 1298 bytes)
15/02/01 22:55:28 INFO TaskSetManager: Starting task 2.0 in stage 18.0 (TID 32, localhost, PROCESS_LOCAL, 1298 bytes)
15/02/01 22:55:28 INFO Executor: Running task 1.0 in stage 18.0 (TID 31)
15/02/01 22:55:28 INFO Executor: Running task 0.0 in stage 18.0 (TID 30)
15/02/01 22:55:28 INFO Executor: Running task 2.0 in stage 18.0 (TID 32)
15/02/01 22:55:28 INFO PythonRDD: Times: total = 42, boot = -21023, init = 21065, finish = 0
15/02/01 22:55:28 INFO PythonRDD: Times: total = 53, boot = -21217, init = 21270, finish = 0
15/02/01 22:55:28 INFO Executor: Finished task 1.0 in stage 18.0 (TID 31). 901 bytes result sent to driver
15/02/01 22:55:28 INFO TaskSetManager: Finished task 1.0 in stage 18.0 (TID 31) in 69 ms on localhost (1/3)
15/02/01 22:55:28 INFO Executor: Finished task 2.0 in stage 18.0 (TID 32). 901 bytes result sent to driver
15/02/01 22:55:28 INFO PythonRDD: Times: total = 56, boot = -21022, init = 21078, finish = 0
15/02/01 22:55:28 INFO TaskSetManager: Finished task 2.0 in stage 18.0 (TID 32) in 88 ms on localhost (2/3)
15/02/01 22:55:28 INFO Executor: Finished task 0.0 in stage 18.0 (TID 30). 901 bytes result sent to driver
15/02/01 22:55:28 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 30) in 106 ms on localhost (3/3)
15/02/01 22:55:28 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
15/02/01 22:55:28 INFO DAGScheduler: Stage 18 (reduceByKey at <stdin>:1) finished in 0.107 s
15/02/01 22:55:28 INFO DAGScheduler: looking for newly runnable stages
15/02/01 22:55:28 INFO DAGScheduler: running: Set()
15/02/01 22:55:28 INFO DAGScheduler: waiting: Set(Stage 19)
15/02/01 22:55:28 INFO DAGScheduler: failed: Set()
15/02/01 22:55:28 INFO DAGScheduler: Missing parents for Stage 19: List()
15/02/01 22:55:28 INFO DAGScheduler: Submitting Stage 19 (PythonRDD[29] at count at <stdin>:1), which is now runnable
15/02/01 22:55:28 INFO MemoryStore: ensureFreeSpace(5736) called with curMem=22751, maxMem=278302556
15/02/01 22:55:28 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 5.6 KB, free 265.4 MB)
15/02/01 22:55:28 INFO MemoryStore: ensureFreeSpace(4240) called with curMem=28487, maxMem=278302556
15/02/01 22:55:28 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 4.1 KB, free 265.4 MB)
15/02/01 22:55:28 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on localhost:43784 (size: 4.1 KB, free: 265.4 MB)
15/02/01 22:55:28 INFO BlockManagerMaster: Updated info of block broadcast_15_piece0
15/02/01 22:55:28 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:838
15/02/01 22:55:28 INFO DAGScheduler: Submitting 3 missing tasks from Stage 19 (PythonRDD[29] at count at <stdin>:1)
15/02/01 22:55:28 INFO TaskSchedulerImpl: Adding task set 19.0 with 3 tasks
15/02/01 22:55:28 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 33, localhost, PROCESS_LOCAL, 1056 bytes)
15/02/01 22:55:28 INFO TaskSetManager: Starting task 1.0 in stage 19.0 (TID 34, localhost, PROCESS_LOCAL, 1056 bytes)
15/02/01 22:55:28 INFO TaskSetManager: Starting task 2.0 in stage 19.0 (TID 35, localhost, PROCESS_LOCAL, 1056 bytes)
15/02/01 22:55:28 INFO Executor: Running task 2.0 in stage 19.0 (TID 35)
15/02/01 22:55:28 INFO Executor: Running task 1.0 in stage 19.0 (TID 34)
15/02/01 22:55:28 INFO Executor: Running task 0.0 in stage 19.0 (TID 33)
15/02/01 22:55:28 INFO ShuffleBlockFetcherIterator: Getting 3 non-empty blocks out of 3 blocks
15/02/01 22:55:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/02/01 22:55:28 INFO ShuffleBlockFetcherIterator: Getting 3 non-empty blocks out of 3 blocks
15/02/01 22:55:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/02/01 22:55:28 INFO ShuffleBlockFetcherIterator: Getting 3 non-empty blocks out of 3 blocks
15/02/01 22:55:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/02/01 22:55:28 INFO PythonRDD: Times: total = 49, boot = -45, init = 94, finish = 0
15/02/01 22:55:28 INFO PythonRDD: Times: total = 44, boot = -20, init = 63, finish = 1
15/02/01 22:55:28 INFO Executor: Finished task 1.0 in stage 19.0 (TID 34). 902 bytes result sent to driver
15/02/01 22:55:28 INFO Executor: Finished task 2.0 in stage 19.0 (TID 35). 902 bytes result sent to driver
15/02/01 22:55:28 INFO PythonRDD: Times: total = 57, boot = -19, init = 75, finish = 1
15/02/01 22:55:28 INFO TaskSetManager: Finished task 1.0 in stage 19.0 (TID 34) in 75 ms on localhost (1/3)
15/02/01 22:55:28 INFO TaskSetManager: Finished task 2.0 in stage 19.0 (TID 35) in 94 ms on localhost (2/3)
15/02/01 22:55:28 INFO Executor: Finished task 0.0 in stage 19.0 (TID 33). 902 bytes result sent to driver
15/02/01 22:55:28 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 33) in 104 ms on localhost (3/3)
15/02/01 22:55:28 INFO DAGScheduler: Stage 19 (count at <stdin>:1) finished in 0.110 s
15/02/01 22:55:28 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
15/02/01 22:55:28 INFO DAGScheduler: Job 13 finished: count at <stdin>:1, took 0.267077 s
3
>>> 
>>> 
>>> reduced_rdd3.take(reduced_rdd3.count())
15/02/01 22:55:37 INFO SparkContext: Starting job: count at <stdin>:1
15/02/01 22:55:37 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 162 bytes
15/02/01 22:55:37 INFO DAGScheduler: Got job 14 (count at <stdin>:1) with 3 output partitions (allowLocal=false)
15/02/01 22:55:37 INFO DAGScheduler: Final stage: Stage 21(count at <stdin>:1)
15/02/01 22:55:37 INFO DAGScheduler: Parents of final stage: List(Stage 20)
15/02/01 22:55:37 INFO DAGScheduler: Missing parents: List()
15/02/01 22:55:37 INFO DAGScheduler: Submitting Stage 21 (PythonRDD[30] at count at <stdin>:1), which has no missing parents
15/02/01 22:55:37 INFO MemoryStore: ensureFreeSpace(5736) called with curMem=32727, maxMem=278302556
15/02/01 22:55:37 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 5.6 KB, free 265.4 MB)
15/02/01 22:55:37 INFO MemoryStore: ensureFreeSpace(4240) called with curMem=38463, maxMem=278302556
15/02/01 22:55:37 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 4.1 KB, free 265.4 MB)
15/02/01 22:55:37 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on localhost:43784 (size: 4.1 KB, free: 265.4 MB)
15/02/01 22:55:37 INFO BlockManagerMaster: Updated info of block broadcast_16_piece0
15/02/01 22:55:37 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:838
15/02/01 22:55:37 INFO DAGScheduler: Submitting 3 missing tasks from Stage 21 (PythonRDD[30] at count at <stdin>:1)
15/02/01 22:55:37 INFO TaskSchedulerImpl: Adding task set 21.0 with 3 tasks
15/02/01 22:55:37 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 36, localhost, PROCESS_LOCAL, 1056 bytes)
15/02/01 22:55:37 INFO TaskSetManager: Starting task 1.0 in stage 21.0 (TID 37, localhost, PROCESS_LOCAL, 1056 bytes)
15/02/01 22:55:37 INFO TaskSetManager: Starting task 2.0 in stage 21.0 (TID 38, localhost, PROCESS_LOCAL, 1056 bytes)
15/02/01 22:55:37 INFO Executor: Running task 0.0 in stage 21.0 (TID 36)
15/02/01 22:55:37 INFO Executor: Running task 1.0 in stage 21.0 (TID 37)
15/02/01 22:55:37 INFO Executor: Running task 2.0 in stage 21.0 (TID 38)
15/02/01 22:55:37 INFO ShuffleBlockFetcherIterator: Getting 3 non-empty blocks out of 3 blocks
15/02/01 22:55:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/02/01 22:55:37 INFO ShuffleBlockFetcherIterator: Getting 3 non-empty blocks out of 3 blocks
15/02/01 22:55:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/02/01 22:55:37 INFO ShuffleBlockFetcherIterator: Getting 3 non-empty blocks out of 3 blocks
15/02/01 22:55:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/02/01 22:55:37 INFO PythonRDD: Times: total = 54, boot = -8662, init = 8716, finish = 0
15/02/01 22:55:37 INFO PythonRDD: Times: total = 61, boot = -8716, init = 8777, finish = 0
15/02/01 22:55:37 INFO PythonRDD: Times: total = 42, boot = -8674, init = 8716, finish = 0
15/02/01 22:55:37 INFO Executor: Finished task 2.0 in stage 21.0 (TID 38). 902 bytes result sent to driver
15/02/01 22:55:37 INFO Executor: Finished task 1.0 in stage 21.0 (TID 37). 902 bytes result sent to driver
15/02/01 22:55:37 INFO Executor: Finished task 0.0 in stage 21.0 (TID 36). 902 bytes result sent to driver
15/02/01 22:55:37 INFO TaskSetManager: Finished task 2.0 in stage 21.0 (TID 38) in 125 ms on localhost (1/3)
15/02/01 22:55:37 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 36) in 144 ms on localhost (2/3)
15/02/01 22:55:37 INFO TaskSetManager: Finished task 1.0 in stage 21.0 (TID 37) in 146 ms on localhost (3/3)
15/02/01 22:55:37 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
15/02/01 22:55:37 INFO DAGScheduler: Stage 21 (count at <stdin>:1) finished in 0.155 s
15/02/01 22:55:37 INFO DAGScheduler: Job 14 finished: count at <stdin>:1, took 0.197338 s
15/02/01 22:55:37 INFO SparkContext: Starting job: runJob at PythonRDD.scala:344
15/02/01 22:55:37 INFO DAGScheduler: Got job 15 (runJob at PythonRDD.scala:344) with 1 output partitions (allowLocal=true)
15/02/01 22:55:37 INFO DAGScheduler: Final stage: Stage 23(runJob at PythonRDD.scala:344)
15/02/01 22:55:37 INFO DAGScheduler: Parents of final stage: List(Stage 22)
15/02/01 22:55:37 INFO DAGScheduler: Missing parents: List()
15/02/01 22:55:37 INFO DAGScheduler: Submitting Stage 23 (PythonRDD[32] at RDD at PythonRDD.scala:43), which has no missing parents
15/02/01 22:55:37 INFO MemoryStore: ensureFreeSpace(5136) called with curMem=42703, maxMem=278302556
15/02/01 22:55:37 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 5.0 KB, free 265.4 MB)
15/02/01 22:55:37 INFO MemoryStore: ensureFreeSpace(3818) called with curMem=47839, maxMem=278302556
15/02/01 22:55:37 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.7 KB, free 265.4 MB)
15/02/01 22:55:37 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on localhost:43784 (size: 3.7 KB, free: 265.4 MB)
15/02/01 22:55:37 INFO BlockManagerMaster: Updated info of block broadcast_17_piece0
15/02/01 22:55:37 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:838
15/02/01 22:55:37 INFO DAGScheduler: Submitting 1 missing tasks from Stage 23 (PythonRDD[32] at RDD at PythonRDD.scala:43)
15/02/01 22:55:37 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks
15/02/01 22:55:37 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 39, localhost, PROCESS_LOCAL, 1056 bytes)
15/02/01 22:55:37 INFO Executor: Running task 0.0 in stage 23.0 (TID 39)
15/02/01 22:55:37 INFO ShuffleBlockFetcherIterator: Getting 3 non-empty blocks out of 3 blocks
15/02/01 22:55:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/02/01 22:55:37 INFO PythonRDD: Times: total = 41, boot = -107, init = 148, finish = 0
15/02/01 22:55:37 INFO Executor: Finished task 0.0 in stage 23.0 (TID 39). 914 bytes result sent to driver
15/02/01 22:55:37 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 39) in 52 ms on localhost (1/1)
15/02/01 22:55:37 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
15/02/01 22:55:37 INFO DAGScheduler: Stage 23 (runJob at PythonRDD.scala:344) finished in 0.053 s
15/02/01 22:55:37 INFO DAGScheduler: Job 15 finished: runJob at PythonRDD.scala:344, took 0.071966 s
15/02/01 22:55:37 INFO SparkContext: Starting job: runJob at PythonRDD.scala:344
15/02/01 22:55:37 INFO DAGScheduler: Got job 16 (runJob at PythonRDD.scala:344) with 2 output partitions (allowLocal=true)
15/02/01 22:55:37 INFO DAGScheduler: Final stage: Stage 25(runJob at PythonRDD.scala:344)
15/02/01 22:55:37 INFO DAGScheduler: Parents of final stage: List(Stage 24)
15/02/01 22:55:37 INFO DAGScheduler: Missing parents: List()
15/02/01 22:55:37 INFO DAGScheduler: Submitting Stage 25 (PythonRDD[33] at RDD at PythonRDD.scala:43), which has no missing parents
15/02/01 22:55:37 INFO MemoryStore: ensureFreeSpace(5136) called with curMem=51657, maxMem=278302556
15/02/01 22:55:37 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 5.0 KB, free 265.4 MB)
15/02/01 22:55:37 INFO MemoryStore: ensureFreeSpace(3818) called with curMem=56793, maxMem=278302556
15/02/01 22:55:37 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 3.7 KB, free 265.4 MB)
15/02/01 22:55:37 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on localhost:43784 (size: 3.7 KB, free: 265.4 MB)
15/02/01 22:55:37 INFO BlockManagerMaster: Updated info of block broadcast_18_piece0
15/02/01 22:55:37 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:838
15/02/01 22:55:37 INFO DAGScheduler: Submitting 2 missing tasks from Stage 25 (PythonRDD[33] at RDD at PythonRDD.scala:43)
15/02/01 22:55:37 INFO TaskSchedulerImpl: Adding task set 25.0 with 2 tasks
15/02/01 22:55:37 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 40, localhost, PROCESS_LOCAL, 1056 bytes)
15/02/01 22:55:37 INFO TaskSetManager: Starting task 1.0 in stage 25.0 (TID 41, localhost, PROCESS_LOCAL, 1056 bytes)
15/02/01 22:55:37 INFO Executor: Running task 0.0 in stage 25.0 (TID 40)
15/02/01 22:55:37 INFO Executor: Running task 1.0 in stage 25.0 (TID 41)
15/02/01 22:55:37 INFO ShuffleBlockFetcherIterator: Getting 3 non-empty blocks out of 3 blocks
15/02/01 22:55:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/02/01 22:55:37 INFO ShuffleBlockFetcherIterator: Getting 3 non-empty blocks out of 3 blocks
15/02/01 22:55:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/02/01 22:55:37 INFO PythonRDD: Times: total = 40, boot = -201, init = 241, finish = 0
15/02/01 22:55:37 INFO Executor: Finished task 1.0 in stage 25.0 (TID 41). 911 bytes result sent to driver
15/02/01 22:55:37 INFO TaskSetManager: Finished task 1.0 in stage 25.0 (TID 41) in 82 ms on localhost (1/2)
15/02/01 22:55:37 INFO PythonRDD: Times: total = 68, boot = -181, init = 248, finish = 1
15/02/01 22:55:37 INFO Executor: Finished task 0.0 in stage 25.0 (TID 40). 911 bytes result sent to driver
15/02/01 22:55:37 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 40) in 105 ms on localhost (2/2)
15/02/01 22:55:37 INFO DAGScheduler: Stage 25 (runJob at PythonRDD.scala:344) finished in 0.111 s
15/02/01 22:55:37 INFO DAGScheduler: Job 16 finished: runJob at PythonRDD.scala:344, took 0.129297 s
15/02/01 22:55:37 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool 
[('k2', -3), ('k1', 3), ('k3', 4)]
>>> 
>>> 
>>> 

</code>
