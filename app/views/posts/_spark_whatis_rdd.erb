<code class='python'>mydata = [10, 20, 30, 40, 50]
myrdd  = sc.parallelize(data)
# I now have an RDD which Spark can chew on using a call to myrdd.reduce(lexp)
# Mem:
# lexp is an anonymous function (AKA lambda) which has 2 inputs fed to some
# calculation syntax.

# Demo:
# myrdd.reduce(lambda a1, a2: a1 + a2)
</code>