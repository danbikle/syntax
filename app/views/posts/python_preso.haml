.q1 Question:
.q2 What is 2016-01-18 Python Meetup?
.tags
  %span.tagtag Tags:
  %a.xtag(href='/tags/python') python
  %a.xtag(href='/tags/numpy') numpy
  %a.xtag(href='/tags/pandas') pandas
  %a.xtag(href='/tags/sklearn') sklearn
  %a.xtag(href='/tags/crontab') crontab
  %a.xtag(href='/tags/linux') linux
  %a.xtag(href='/tags/baypiggies') baypiggies
  %a.xtag(href='/tags/meetup') meetup
  %a.xtag(href='/tags/yahoo') yahoo
  %a.xtag(href='/tags/gspc') gspc
  
%br/

This is/will be a presentation at the Python Meetup on 2016-01-18 at Hacker Dojo.

%br/
%br/
You can RSVP at this URL:

%br/
%br/

%a(href='http://www.meetup.com/BayPIGgies/events/227481217' target='x')
  http://www.meetup.com/BayPIGgies/events/227481217

%br/
%br/

Currently this page is UNDER CONSTRUCTION.

%br/
%br/

%h2 Use Case Discussion
%ul
  %li Ted the Trader lives in California; he uses his lunch hour to rebalance his position in SPY once a day.
  %li On a Tuesday at 12:50 PM he sees a new prediction appear at http://www.pyspy.info and that prediction is bullish.
  %li Ted looks at his current position of SPY inside his brokerage account and sees that it is bearish.
  %li At 12:55 PM Ted rebalances his position of SPY so that it is bullish.
  %li Ted is done trading for that Tuesday.

%h2 Development Environment Setup

I use Ubuntu 14 to develop and run pyspy.

%br/
%br/

I like this release:

%br/
%br/

%a(href='http://releases.ubuntu.com/14.04/ubuntu-14.04.3-desktop-amd64.iso' target='x')
  http://releases.ubuntu.com/14.04/ubuntu-14.04.3-desktop-amd64.iso

%br/
%br/

I prefer to run Ubuntu on a newish-laptop.  I can usually replace
windows-10 with Ubuntu in about 9 minutes.  After the initial Ubuntu
bootup, I need about 30 minutes to install all the latest Ubuntu updates.

%br/
%br/

A suitable newish-laptop should cost less than $500.
Ubuntu friendly CPUs are i3, i5, i7, or A10; avoid Pentium.

%br/
%br/

If Ubuntu fails to install on the newish-laptop, I return the laptop to the store for a refund.

%br/
%br/

Stores which behave well: Costco, Walmart, OfficeDepot, and BestBuy .

%br/
%br/

If I want to leave windows-10 on the laptop, I can use a USB drive to power Linux on the laptop while windows-10 sleeps.

%br/
%br/

Another way to run Ubuntu is inside of a VirtualBox on my Mac.

%br/
%br/

It is possible to run Ubuntu inside of a VirtualBox inside of windows-10 but that setup often displays bad behavior.

%br/
%br/

It is possible to build a Mac-only development environment but that environment becomes more cumbersome as the ML-app grows in complexity.

%br/
%br/

It is possible to build a complex ML-app which runs on both Mac and Ubuntu but my attitude is 'why bother?'

%br/
%br/

Instead I just pick the OS which will support the production-app, in this case Ubuntu, and then use that OS to host my development environment.

%br/
%br/

During installation the Ubuntu wizard will ask my name and where I live.

%br/
%br/

For the pyspy-app the proper name is: 'ann'

%br/
%br/

Ubuntu will then create a privileged account named: 'ann'

%br/
%br/

Also I should tell the wizard that I live in California so that predictions generated at 12:50pm will be 10 minutes before market close at 13:00 pm.

%br/
%br/

After I install Ubuntu, I run shell commands to enhance it:
%pre
  =render 'python_preso10'

With that behind me I should install ta-lib:
%pre
  =render 'python_preso11'

After I install ta-lib, I should install python:
%pre
  =render 'python_preso12'

Then, I should pip install TA-lib:
%pre
  %code.bash
    pip install TA-Lib
    python -c 'import numpy, talib'

%h2 Data Acquistion (Automated)
For pyspy, automated data acquistion is simple.
%ul
  %li Create a folder for data
  %li Clone a repo with a script to copy data into the folder
  %li Create a cron entry to run the script at 12:50 M-F.
  %li I list syntax below to do this:

%pre
  =render 'python_preso13'

The crontab in the above example assumes that you are in the California timezone.

%br/
%br/

If you are in a different timezone, study the comments in ~/pyspy/crontab.txt to see what you should change.

%h2 Data Acquistion (Manual at Night)

To manually acquire data, run this shell script:

%pre
  %code.bash ~/pyspy/bin/night.bash

I captured some screen output when I did that:

%pre
  =render 'python_preso14'

%h2 Data Acquistion (Manual at Near Calif-Noon)

When the market is near close (California lunch hour) sometimes I will manually acquire data:

%pre
  %code.bash ~/pyspy/bin/noon50.bash

I captured some screen output when I did that:

%pre
  =render 'python_preso15'

The script
%a(href='https://github.com/danbikle/pyspy/blob/master/bin/noon50.bash' target='g')
  noon50.bash
is an enhanced version of
%a(href='https://github.com/danbikle/pyspy/blob/master/bin/night.bash' target='g')
  night.bash

%br/
%br/

The script
%a(href='https://github.com/danbikle/pyspy/blob/master/bin/noon50.bash' target='g')
  noon50.bash
gets the most recent price by scraping a web-page with python.

%br/
%br/

After you study 
%a(href='https://github.com/danbikle/pyspy/blob/master/bin/noon50.bash' target='g')
  noon50.bash
and
%a(href='https://github.com/danbikle/pyspy/blob/master/bin/night.bash' target='g')
  night.bash
you should notice that they do more than just collect data.
They also do the following tasks:
%ul
  %li Create Features
  %li Build a model with scikit-learn
  %li Build Visualization of OOS predictions

The above three topics are discussed below.

%br/
%br/

%h2 Feature Creation with Python

When I talk with stock-traders about how they decide to open or close a position,
often they will describe an example of a price chart:

%br/
%br/

%img(src='/images/python_preso16.png')

%br/
%br/

For example they might overlay a simple-moving-average on top of a price-time-series and then make decisions based on rules which they believe are true.

%br/
%br/

An example of a rule might be this statement:
When price is below a 5-day-moving average, I should be bullish.

%br/
%br/

A Python programmer might look at that rule and ask,
"How to use Python to build a 5-day-moving average and then compare it to the price?"

%br/
%br/

The answer is, it is relatively easy to do this calculation in Python.

%br/
%br/

As a product manager I would describe how I want to interact with this calculation.

%br/
%br/

Currently, I would be happy if the calculation was placed within a simple 2D Python-List:

%pre
  %code.python
    [
    ["2015-12-01",2102.63,price_over5d_sma]
    ,["2015-12-02",2079.51,price_over5d_sma]
    ,["2015-12-03",2049.62,price_over5d_sma]
    ,["2015-12-04",2091.69,price_over5d_sma]
    ]

As a programmer I would look at the above example and then write this python code:

%pre
  =render 'python_preso16'

%br/
%br/

%h2 Feature Creation with NumPy

As a Product Manager I would want more features than just current_price / mvg_avg.

%br/
%br/

The code below demonstrates how to create more features from prices:

%pre
  =render 'python_preso17'

%br/
%br/

%h2 Feature Creation with TA-lib

The code below demonstrates how to create more features from Bollinger Bands calculated using TA-lib:

%pre
  =render 'python_preso18'

%br/
%br/

%h2 Feature Visualization with Pandas

Pandas offers a syntax for applying predicates to tabular data.

%br/
%br/

I can use this syntax to visualize dependency between features and future deltas:

%pre
  =render 'python_preso19'

%h2 Model Building with scikit-learn

On github, a link to the Python script I use to build Logistic Regression model from my features is listed below:

%br/
%br/

%a(href='https://github.com/danbikle/pyspy/blob/master/py/learn_test.py' target='x')
  https://github.com/danbikle/pyspy/blob/master/py/learn_test.py

%br/
%br/

The syntax which creates a model is 3 simple lines of Python:
%pre
  %code.python
    from sklearn import linear_model
    lrmodel = linear_model.LogisticRegression()
    lrmodel.fit(x_train_a, label_train_a)

Much of my effort behind this script supported creation of the the two Numpy Arrays which I feed to lrmodel.fit():

%ul
  %li x_train_a
  %li label_train_a

The first array, x_train_a, is a simple array of features which I discussed earlier in this page.

%br/
%br/

The second array, label_train_a, is an array of what I call 'labels'.
A label is one of two values: True or False

%br/
%br/

Both arrays are created from all observations between 1987 and 2015 which is about (2015-1987)*252 observations.

%br/
%br/

My intent here is to create a model which will help me predict if a future observation is above the training median, True or False?

%br/
%br/

In Python terms, a future observation is a simple array like this: 

%pre
  %code.python [pctlag1, pctlag2, pctlag4, pctlag8, upf, lowf]

An actual future observation might look like this:

%pre
  %code.python [-0.127,0.791,0.000,-1.194,-2.056, 1.002, 1.004]

%h2 Prediction of Out-of-Sample (OOS) data

After I create the model, I have a Python object called lrmodel.

%br/
%br/

I can use this model to make predictions with appropriate calls to the sklearn API.

%br/
%br/

For example I could make a single prediction with syntax like this:

%pre
  %code.python
    xoos_a      = [-0.127,0.791,0.000,-1.194,-2.056, 1.002, 1.004]
    xf_a        = xoos_a.astype(float)
    xr_a        = xf_a.reshape(1, -1)
    aprediction = lrmodel.predict_proba(xr_a)[0,1]

Then I will see my prediction in the variable, aprediction.

%br/
%br/

The prediction should be a value between 0 and 1.

%br/
%br/

A value of 0.55 is a signal that the model is 55% confident that the label for the observation:

%pre
  %code.python [-0.127,0.791,0.000,-1.194,-2.056, 1.002, 1.004]
will be True.

I am interested in more than one prediction;
I want all the predictions for 2015.

%br/
%br/

To get those I use a loop:

%pre
  =render 'python_preso20'  

%h2 Visualization of OOS predictions

After I collected the 2015 predictions, I had some questions:

%ul
  %li How accurate where the predictions?
  %li How 'effective' where they?
  %li How to best visualize their effectiveness?

Measuring accuracy is easy.

%br/
%br/

For example if I have 10 predictions and 6 of them are correct (AKA 'True') then I have 60% accuracy.

%br/
%br/

Usually I like to categorize my accuracy calculations into Postive accuracy and Negative accuracy.

If I have a Negative accuracy of 60% and a Postive accuracy of 50% then I see that my model
is better at predicting when the market will go down.

%br/
%br/

That would be useful information if I am gambling money with these predictions.

%br/
%br/

When I study the above Python syntax I see that the 2015 predictions accumulate in a list named predictions_l via this line:

%pre
  %code.python predictions_l.append(aprediction)

%br/
%br/

I rely on Pandas to help me calculate prediction accuracy.

%br/
%br/

I start this effort by transforming predictions_l into +1 or -1 using syntax like this:

%pre
  %code.python test_df['pdir'] = [np.sign(prediction-0.5) for prediction in predictions_l] 

The above line captures prediction 'direction' with a column named 'pdir' in a Pandas DataFrame.

%br/
%br/

Next, I capture the actual priced direction using syntax like this:

%pre
  %code.python
    cp_l   = list(test_df['cp'].values)
    lead_l = cp_l[1:] + [cp_l[len(cp_l)-1]]
    lead_a = np.array(lead_l) - np.array(cp_l)
    test_df['actual_dir'] = [np.sign(lead_delta) for lead_delta in lead_a]

Earlier in this page I showed some Pandas predicate syntax to 
visualize dependency between features and future deltas.

%br/
%br/

Now I show more Pandas predicate syntax to calculate accuracies:
%pre
  %code.python
    # I should count positive predictions.
    posp_df  = test_df[['pdir','actual_dir']][test_df['pdir'] == 1]
    # I should count true positive predictions.
    tposp_df = posp_df[posp_df['actual_dir'] == 1]
    # I should calculate positive accuracy.
    pos_acc  = 100.0 * len(tposp_df)/len(posp_df)
    # I should count negative predictions.
    negp_df  = test_df[['pdir','actual_dir']][test_df['pdir'] == -1]
    # I should count true negative predictions.
    tnegp_df = negp_df[negp_df['actual_dir'] == -1]
    # I should calculate negative accuracy.
    neg_acc  = 100.0 * len(tnegp_df)/len(negp_df)
    # I should calculate combined accuracy
    com_acc  = 100.0 * (len(tposp_df)+len(tnegp_df))/len(test_df)



%br/
%br/

%br/
%br/

%br/
%br/

More topics under construction:


%h2 Build  the ML Application with Django
%h2 Deploy  the ML Application to Heroku
%h2 Operate the ML Application on Heroku
