.q1 Question:
.q2 In Apache Spark how to create RDD from text file?
.tags
  %span.tagtag Tags:
  %a.xtag(href='/tags/apache') apache
  %a.xtag(href='/tags/spark') spark
  %a.xtag(href='/tags/rdd') rdd
  %a.xtag(href='/tags/python') python
  %a.xtag(href='/tags/sparkcontext') sparkcontext
  %a.xtag(href='/tags/sc') sc
  %a.xtag(href='/tags/resilient') resilient
  %a.xtag(href='/tags/distributed') distributed
  %a.xtag(href='/tags/dataset') dataset

%br/
Early in my study of Spark I found a great demo in the quick-start.html 
which almost answered my question of how to load CSV data into Spark.
%br/
%br/
First, the acronym: 'RDD' comes from 'Resilient Distributed Dataset'.
%br/
%br/
I think of it as data in a form which Spark understands.
%br/
%br/
Assuming I have Spark installed in my home directory here:

%pre
  %code.bash ~/spark/

I should see a script here:

%pre
  %code.bash ~/spark/bin/pyspark

I can run it and it and Spark should give me a Python prompt which I can use to interact
with Spark:

%pre
  =render 'spark_howto_txt2rdd'

Next, I use wget to get a copy of a CSV file from Yahoo:

%pre
  =render 'spark_howto_txt2rdd2'

Then, I use one line of code to create a Spark RDD from the CSV file:

%pre
  =render 'spark_howto_txt2rdd3'
%br/

According to quick-start.html I can make calls to my new RDD:

%pre
  =render 'spark_howto_txt2rdd4'

So, creating a Spark RDD from a CSV file is easy.

