<code class='bash'>

The first line in the doc is this:
val sc: SparkContext // An existing SparkContext.


When I type the above line into bin/spark-shell
I get an error which is a bit of a problem:

scala> 
scala> 
scala> val sc: SparkContext
val sc: SparkContext
&lt;console>:10: error: not found: type SparkContext
       val sc: SparkContext
               ^
scala> 
scala> 



I consider this to be a documentation bug.

I do not need the above line; it should not be in the doc.

I see that sc is already defined during startup of the spark-shell:


scala> 
scala> sc
res0: org.apache.spark.SparkContext = org.apache.spark.SparkContext@4b88807b

scala> 
scala> 

Next I typed more lines:

scala> 
scala> val sqlContext = new org.apache.spark.sql.SQLContext(sc)
sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@cc98901
scala> 
scala> 

scala> 
scala> // createSchemaRDD is used to implicitly convert an RDD to a SchemaRDD.
scala> import sqlContext.createSchemaRDD
scala> 


scala> 
scala> val sqlContext = new org.apache.spark.sql.SQLContext(sc)
sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@1d5dcc8c
scala> 


scala> // createSchemaRDD is used to implicitly convert an RDD to a SchemaRDD.
scala> import sqlContext.createSchemaRDD
scala> 


scala> // Define the schema using a case class.
scala> // Note: Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit,
scala> // you can use custom classes that implement the Product interface.
scala> case class Person(name: String, age: Int)
defined class Person
scala> 
scala> 

Next, I typed this:
scala> 
scala> // Create an RDD of Person objects and register it as a table.
scala> val people = sc.textFile("examples/src/main/resources/people.txt").map(_.split(",")).map(p => Person(p(0), p(1).trim.toInt))
people: org.apache.spark.rdd.RDD[Person] = MappedRDD[3] at map at &lt;console>:21


I see from the syntax and the output that I had just used class Person.

I translate the above output to English:

I have a new object named people.
It is of type org.apache.spark.rdd.RDD[Person]
Notice the above type makes use of Person.
Here Person might be acting as a key.
This people object equals MappedRDD[3]

When I study 
examples/src/main/resources/people.txt
I see only three lines:

Michael, 29
Andy, 30
Justin, 19


The syntax which I used to create people is interesting.

I translate it to English:

val people = sc.textFile()

The val is a token which is often used when I create a new variable.

According to:

https://twitter.github.io/scala_school/basics.html

I cannot change the binding to the people object.

If I want to change the binding, I should use var instead of val.


I translate expression below to English:

val people = sc.textFile("examples/src/main/resources/people.txt")

I understand it to mean: 
SparkContext should try to read this text file.

Google sent me here:
https://spark.apache.org/docs/1.2.1/api/scala/index.html#org.apache.spark.SparkContext

The above URL says,
Read this text file and return it as an RDD of Strings.


I translate expression below to English:

val people = sc.textFile("examples/src/main/resources/people.txt").map(_.split(","))

I have an RDD of strings which is a collection of strings.  Now operate on it with .map()

When I see .map() I see a mechanism which builds a loop and then feeds elements into the loop.

In this case each element is a string.

Inside the loop each element gets chewed on and then spit out.

In the case, the chewing equates to this expression: 
_.split(",")

I assume that underscore is a variable which contains an input string.

I assume that .split(",") will split a string into substrings at each comma.

Then once all the elements are processed, I end up with a new RDD.

In this case I end up with an RDD of split up strings.

</code>
