<code class='bash'>

The first line in the doc is this:
val sc: SparkContext // An existing SparkContext.


When I type the above line into bin/spark-shell
I get an error which is a bit of a problem:

scala> 
scala> 
scala> val sc: SparkContext
val sc: SparkContext
&lt;console>:10: error: not found: type SparkContext
       val sc: SparkContext
               ^
scala> 
scala> 



I consider this to be a documentation bug.

I do not need the above line; it should not be in the doc.

I see that sc is already defined during startup of the spark-shell:


scala> 
scala> sc
res0: org.apache.spark.SparkContext = org.apache.spark.SparkContext@4b88807b

scala> 
scala> 

Next I typed more lines:

scala> 
scala> val sqlContext = new org.apache.spark.sql.SQLContext(sc)
sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@cc98901
scala> 
scala> 

scala> 
scala> // createSchemaRDD is used to implicitly convert an RDD to a SchemaRDD.
scala> import sqlContext.createSchemaRDD
scala> 


scala> 
scala> val sqlContext = new org.apache.spark.sql.SQLContext(sc)
sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@1d5dcc8c
scala> 


scala> // createSchemaRDD is used to implicitly convert an RDD to a SchemaRDD.
scala> import sqlContext.createSchemaRDD
scala> 


scala> // Define the schema using a case class.
scala> // Note: Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit,
scala> // you can use custom classes that implement the Product interface.
scala> case class Person(name: String, age: Int)
defined class Person
scala> 
scala> 

Next, I typed this:
scala> 
scala> // Create an RDD of Person objects and register it as a table.
scala> val people = sc.textFile("examples/src/main/resources/people.txt").map(_.split(",")).map(p => Person(p(0), p(1).trim.toInt))
people: org.apache.spark.rdd.RDD[Person] = MappedRDD[3] at map at &lt;console>:21


I see from the syntax and the output that I had just used class Person.

I translate the above output to English:

I have a new object named people.
It is of type org.apache.spark.rdd.RDD[Person]
Notice the above type makes use of Person.
Here Person might be acting as a key.
This people object equals MappedRDD[3]

When I study 
examples/src/main/resources/people.txt
I see only three lines:

Michael, 29
Andy, 30
Justin, 19


The syntax which I used to create people is interesting.

I translate it to English:

val people = sc.textFile()

The val is a token which is often used when I create a new variable.

According to:

https://twitter.github.io/scala_school/basics.html

I cannot change the binding to the people object.

If I want to change the binding, I should use var instead of val.


I translate expression below to English:

val people = sc.textFile("examples/src/main/resources/people.txt")

I understand it to mean: 
SparkContext should try to read this text file.

Google sent me here:
https://spark.apache.org/docs/1.2.1/api/scala/index.html#org.apache.spark.SparkContext

The above URL says,
Read this text file and return it as an RDD of Strings.

Since people is an RDD, I should be able to operate on people with take():

scala> 
scala> 
scala> val people1 = sc.textFile("examples/src/main/resources/people.txt")
people1: org.apache.spark.rdd.RDD[String] = examples/src/main/resources/people.txt MappedRDD[5] at textFile at &lt;console>:19
scala> people1.take(3)
res1: Array[String] = Array(Michael, 29, Andy, 30, Justin, 19)
scala> 
scala> 


I translate expression below to English:

val people = sc.textFile("examples/src/main/resources/people.txt").map(_.split(","))

I have an RDD of strings which is a collection of strings.  Now operate on it with .map()

When I see .map() I see a mechanism which feeds elements into a loop.

In this case each element is a string.

Inside the loop each element gets chewed on and then spit out.

In the case, the 'chewing' is this expression: 
_.split(",")

I assume that underscore is a variable which contains an input string.

I assume that .split(",") will split a string into substrings at each comma.

Then once all the elements are processed, .map() finishes by giving me a new RDD.

In this case I end up with an RDD of split up strings.

I will try this in my bin/spark-shell:

scala> 
scala> 
scala> val people2 = sc.textFile("examples/src/main/resources/people.txt").map(_.split(","))
people2: org.apache.spark.rdd.RDD[Array[String]] = MappedRDD[8] at map at &lt;console>:19
scala> people2.take(3)
res2: Array[Array[String]] = Array(Array(Michael, " 29"), Array(Andy, " 30"), Array(Justin, " 19"))
scala> 
scala> 

I see some subtle behavior here.

My call to .map() transformed an array of strings into an array of arrays.

I suspect that the newline characters in the people.txt file helped make this happen.

This is convenient; I want each line in the text file to become an Array of strings nested 
in a parent Array.


Next I studied this expression:

val people3 = sc.textFile("examples/src/main/resources/people.txt").map(_.split(",")).map(p => Person(p(0), p(1).trim.toInt))

I visualize it as this:

val people3 = array_of_arrays.map(anarray => Person(anarray(0), anarray(1).trim.toInt))

The signature of .map() reminds me of how I understand .map() in Ruby.

When I look at the above expression I can see that people3 will be an Array of Person objects.

I saw this when I ran it:

scala> 
scala> val people3 = sc.textFile("examples/src/main/resources/people.txt").map(_.split(",")).map(p => Person(p(0), p(1).trim.toInt))
people3: org.apache.spark.rdd.RDD[Person] = MappedRDD[12] at map at &lt;console>:21
scala> 
scala> 


I used take() to visualize it:

scala> 
scala> people3.take(3)
res3: Array[Person] = Array(Person(Michael,29), Person(Andy,30), Person(Justin,19))
scala> 
scala> 

Next, I typed another line from the doc:

scala> 
scala> people.registerTempTable("people")
scala> 

It is obvious here that if I want to make calls to the SQL API I need to declare which
tables the API will act on.

The method call suggests that I can declare temporary tables.

I do not know if Spark supports the notion of a permanent table.

The next bit of syntax from the doc is this:

scala> 
scala> val teenagers = sqlContext.sql("SELECT name FROM people WHERE age >= 13 AND age &lt;= 19")
teenagers: org.apache.spark.sql.SchemaRDD = 
SchemaRDD[15] at RDD at SchemaRDD.scala:108
== Query Plan ==
== Physical Plan ==
Project [name#0]
 Filter ((age#1 >= 13) && (age#1 &lt;= 19))
  PhysicalRDD [name#0,age#1], MapPartitionsRDD[13] at mapPartitions at ExistingRDD.scala:36

scala> 
scala> 


It seems obvious that teenagers would have the same structure as people:

scala> 
scala> teenagers.take(1)
res5: Array[org.apache.spark.sql.Row] = Array([Justin])
scala> 

It appears, though, they are different.

The doc offers this syntax as a way to see teenagers:

scala> 
scala> teenagers.map(t => "Name: " + t(0)).collect().foreach(println)
Name: Justin
scala> 

I think the .collect() converts the RDD from .map() into an Array.

Then I can use .foreach(println) to print each string in the Array.


In the above discussion I assumed that the structure of the people table was known before runtime.

If I know the structure of a table only during runtime, I need to add more syntax.

The doc describes a multi step process:

0. Create RDD from text file
1. Create a StructType object
2. Create rowRDD from RDD
3. Call applySchema to get SchemaRDD
4. registerTempTable
5. .sql()


I walked through the code:

// Create RDD from text file

scala> 
scala> val people = sc.textFile("examples/src/main/resources/people.txt")
people: org.apache.spark.rdd.RDD[String] = examples/src/main/resources/people.txt MappedRDD[26] at textFile at &lt;console>:19

scala> 
scala> 



// Create a StructType object


scala> 
scala> val schemaString = "name age"
schemaString: String = name age
scala> 
scala> 
scala> import org.apache.spark.sql._
scala> 


scala> 
scala> val schema = StructType(schemaString.split(" ").map(fieldName => StructField(fieldName, StringType, true)))
schema: org.apache.spark.sql.catalyst.types.StructType = StructType(ArraySeq(StructField(name,StringType,true), StructField(age,StringType,true)))
scala> 
scala> 


// Create rowRDD from RDD

scala> 
scala> 
scala> val rowRDD = people.map(_.split(",")).map(p => Row(p(0), p(1).trim))
rowRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.catalyst.expressions.Row] = MappedRDD[28] at map at &lt;console>:24
scala> 
scala> 
scala> 

// Call applySchema to get SchemaRDD

scala> 
scala> val sqlContext = new org.apache.spark.sql.SQLContext(sc)
sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@2eddcc47
scala> 
scala> 


scala> 
scala> val peopleSchemaRDD = sqlContext.applySchema(rowRDD, schema)
peopleSchemaRDD: org.apache.spark.sql.SchemaRDD = 
SchemaRDD[29] at RDD at SchemaRDD.scala:108
== Query Plan ==
== Physical Plan ==
PhysicalRDD [name#2,age#3], MappedRDD[28] at map at <console>:24
scala> 
scala> 


// registerTempTable

scala> 
scala> peopleSchemaRDD.registerTempTable("people")
scala> 


// .sql()

scala> 
scala> val results = sqlContext.sql("SELECT name FROM people")
results: org.apache.spark.sql.SchemaRDD = 
SchemaRDD[30] at RDD at SchemaRDD.scala:108
== Query Plan ==
== Physical Plan ==
Project [name#2]
 PhysicalRDD [name#2,age#3], MappedRDD[28] at map at <console>:24

scala> 
scala> 


scala> 
scala> results.collect()
res11: Array[org.apache.spark.sql.Row] = Array([Michael], [Andy], [Justin])
scala> 

scala> 
scala> results.take(3)
res12: Array[org.apache.spark.sql.Row] = Array([Michael], [Andy], [Justin])
scala> 
</code>


