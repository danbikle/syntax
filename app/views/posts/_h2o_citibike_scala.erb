<code class='bash'>
code is here:

https://github.com/h2oai/sparkling-water/blob/master/examples/src/main/scala/org/apache/spark/examples/h2o/CitiBikeSharingDemo.scala

git clone git@github.com:h2oai/sparkling-water.git
cd sparkling-water
./gradlew build -x test



export SPARK_HOME='/home/dan/spark'
export MASTER="local-cluster[3,2,1024]"
bin/sparkling-shell


cd examples/src/main/scala/org/apache/spark/examples/h2o/

dan@hp ~/sw/sw/examples/src/main/scala/org/apache/spark/examples/h2o $ 
dan@hp ~/sw/sw/examples/src/main/scala/org/apache/spark/examples/h2o $ ll
total 56
drwxrwxr-x 2 dan dan 4096 Feb  6 07:14 ./
drwxrwxr-x 3 dan dan 4096 Feb  4 02:44 ../
-rw-rw-r-- 1 dan dan 3816 Feb  4 02:44 AirlinesWithWeatherDemo.scala
-rw-rw-r-- 1 dan dan 5057 Feb  4 02:44 AirlinesWithWeatherDemo2.scala
-rw-rw-r-- 1 dan dan 4782 Feb  6 07:14 CitiBikeSharingDemo.scala
-rw-rw-r-- 1 dan dan 3259 Feb  4 02:44 DeepLearningDemo.scala
-rw-rw-r-- 1 dan dan 3259 Feb  4 02:44 DeepLearningDemoWithoutExtension.scala
-rw-rw-r-- 1 dan dan 3496 Feb  4 02:44 DemoUtils.scala
-rw-rw-r-- 1 dan dan 3051 Feb  4 02:44 ProstateDemo.scala
-rw-rw-r-- 1 dan dan 2050 Feb  4 02:44 SchemaUtils.scala
-rw-rw-r-- 1 dan dan 6948 Feb  4 02:44 Schemas.scala
dan@hp ~/sw/sw/examples/src/main/scala/org/apache/spark/examples/h2o $ 
dan@hp ~/sw/sw/examples/src/main/scala/org/apache/spark/examples/h2o $ 
dan@hp ~/sw/sw/examples/src/main/scala/org/apache/spark/examples/h2o $ 



Here is the code I want to step through:

package org.apache.spark.examples.h2o

import java.io.File

import hex.tree.gbm.GBMModel
import hex.{ModelMetricsSupervised, Model, ModelMetrics, SupervisedModel}
import hex.splitframe.{ShuffleSplitFrame, SplitFrame}
import hex.splitframe.SplitFrameModel.SplitFrameParameters
import org.apache.spark.h2o.H2OContext
import org.apache.spark.sql.{SchemaRDD, SQLContext}
import org.apache.spark.{SparkContext, SparkConf}
import org.apache.spark.examples.h2o.DemoUtils._
import org.joda.time.MutableDateTime
import water.{Key, MRTask}
import water.fvec._

/**
 * Citibike NYC Demo.
 *
 *
 */
object CitiBikeSharingDemo {

  def main(args: Array[String]): Unit = {
    // Configure this application
    val conf: SparkConf = configure("Sparkling Water Meetup: Predict occupation of citi bike station in NYC")

    // Create SparkContext to execute application on Spark cluster
    val sc = new SparkContext(conf)
    val h2oContext = new H2OContext(sc).start()
    import h2oContext._

    implicit val sqlContext = new SQLContext(sc)
    import sqlContext._

    //
    // Load data into H2O by using H2O parser
    //
    val dataf = new DataFrame(new File("/Users/michal/Devel/projects/h2o/repos/h2o2/bigdata/laptop/citibike-nyc/2013-09.csv"))

    //
    // Transform start time to days from Epoch
    //
    val startTimeF = dataf('starttime)
    // Add a new column
    dataf.add(new TimeSplit().doIt(startTimeF))
    println(dataf)

    //
    // Transform DataFrame into SchemaRDD
    //
    val brdd = asSchemaRDD(dataf)

    // Register table and SQL table
    sqlContext.registerRDDAsTable(brdd, "brdd")

    //
    // Do grouping
    //
    val bph = sql(
      """SELECT Days, start_station_id, count(*) bikes
        |FROM brdd
        |GROUP BY Days, start_station_id """.stripMargin)
    println(bph.take(10).mkString("\n"))

    val bphf:DataFrame = bph
    val daysVec = bphf('Days)
    val finalTable = bphf.add(new TimeTransform().doIt(daysVec))
    println(finalTable)

    //
    // Split into train and test parts
    //
    val keys = Array[String]("train.hex", "test.hex", "hold.hex").map(Key.make(_))
    val ratios = Array[Double](0.6, 0.3, 0.1)
    val frs = ShuffleSplitFrame.shuffleSplitFrame(finalTable, keys, ratios, 1234567689L)
    val train = frs(0)
    val test = frs(1)
    val hold = frs(2)

    // Cleanup
    dataf.delete()

    //
    // Launch GBM prediction
    //
    import hex.tree.gbm.GBM
    import hex.tree.gbm.GBMModel.GBMParameters

    val gbmParams = new GBMParameters()
    gbmParams._train = train
    gbmParams._valid = test
    gbmParams._response_column = 'bikes
    gbmParams._ntrees = 500
    gbmParams._max_depth = 6

    val gbm = new GBM(gbmParams)
    val gbmModel = gbm.trainModel.get

    gbmModel.score(train).remove()
    gbmModel.score(test).remove()
    gbmModel.score(hold).remove()

    println(
      s"""
         |r2 on train: ${r2(gbmModel, train)}
         |r2 on test:  ${r2(gbmModel, test)}
         |r2 on hold:  ${r2(gbmModel, hold)}"""".stripMargin)

    sc.stop()
  }

  def r2(model: GBMModel, fr: Frame) =  ModelMetrics.getFromDKV(model, fr).asInstanceOf[ModelMetricsSupervised].r2()

  def basicStats(brdd: SchemaRDD)(implicit sqlContext:SQLContext): Unit = {
    import sqlContext._

    // check Sri's first case
    brdd.first
    brdd.count

    // Register table and SQL table
    sqlContext.registerRDDAsTable(brdd, "brdd")

    val tGBduration = sql("select bikeid, sum(tripduration) from brdd group by bikeid")
    // Sort based on duration
    val bottom10 = tGBduration.sortBy( r => r.getLong(1)).take(10)

    // Get min
    import org.apache.spark.sql.Row
    val minDurationBikeId = tGBduration.min()(Ordering.by[Row, Long](r => r.getLong(1)))

    assert (bottom10(0) == minDurationBikeId)

    val maxDurationBikeId = tGBduration.min()(Ordering.by[Row, Long](r => -r.getLong(1)))

  }

}

class TimeSplit extends MRTask[TimeSplit] {
  def doIt(time: DataFrame):DataFrame =
      DataFrame(doAll(1, time).outputFrame(Array[String]("Days"), null))

  override def map(msec: Chunk, day: NewChunk):Unit = {
    for (i <- 0 until msec.len) {
      day.addNum(msec.at8(i) / (1000 * 60 * 60 * 24)); // Days since the Epoch
    }
  }
}

class TimeTransform extends MRTask[TimeSplit] {
  def doIt(days: DataFrame):DataFrame =
    DataFrame(doAll(2, days).outputFrame(Array[String]("Month", "DayOfWeek"), null))

  override def map(in: Array[Chunk], out: Array[NewChunk]):Unit = {
    val days = in(0)
    val month = out(0)
    val dayOfWeek = out(1)
    val mdt = new MutableDateTime()
    for (i <- 0 until days.len) {
      val msec = days.at8(i) * (1000L*60*60*24)
      mdt.setMillis(msec)
      month.addNum(mdt.getMonthOfYear - 1)
      dayOfWeek.addNum(mdt.getDayOfWeek -1)
    }
  }
}

helloworld,

I'm working with the Feb4 sparkling-water citibike demo.

I started by getting the data from here:

http://www.civicdata.com/dataset/nyc-bike-share-trip-data

I got the 2013-09 csv and put it here:

/tmp/citi.csv

I inspected it with 
head /tmp/citi.csv

I cloned the repo:

cd /tmp/
git clone git@github.com:h2oai/sparkling-water.git
git log -1
commit 635547f2245845e03e573b7868f29886e8a49599
Author: mmalohlava <michal.malohlava@gmail.com>
Date:   Thu Feb 5 19:23:26 2015 -0800

I did this:

cd examples/src/main/scala/org/apache/spark/examples/h2o/

sed -i '/michal/s:/Users/michal/Devel/projects/h2o/repos/h2o2/bigdata/laptop/citibike-nyc/2013-09.csv:/tmp/citi.csv:' CitiBikeSharingDemo.scala


cd /tmp/sparkling-water

export SPARK_HOME="/home/dan/spark"
export MASTER="local-cluster[3,2,1024]"

./gradlew build -x test
./gradlew assemble

bin/run-example.sh CitiBikeSharingDemo

It seemed to do okay for awhile and then I saw many errors

example:

02-06 09:03:35.933 192.168.1.95:54329 9088 #UDP-Recv ERRR: UDP
Receiver error on port 54330java.lang.ArrayIndexOutOfBoundsException:
70

and this:


Exception in thread "main" java.lang.RuntimeException: Cloud size
under 3 at water.H2O.waitForCloudSize(H2O.java:874)

It continued spitting out exception messages and then eventually died.

I tried this:

pkill java
bin/run-example.sh CitiBikeSharingDemo

and it seemed to behave a little better and then issued this:


Exception in thread "main"
org.apache.spark.sql.catalyst.errors.package$TreeNodeException:
Unresolved attributes: 'start_station_id,'start_station_id, tree:
'Aggregate [Days#15,'start_station_id],
[Days#15,'start_station_id,COUNT(1) AS bikes#16L]
 
Subquery brdd
  
SparkLogicalPlan (ExistingRdd
[tripduration#0,starttime#1,stoptime#2,start station id#3,start
station name#4,start station latitude#5,start station longitude#6,end
station id#7,end station name#8,end station latitude#9,end station
longitude#10,bikeid#11,usertype#12,birth year#13,gender#14,Days#15],
H2OSchemaRDD[6] at H2OSchemaRDD at H2OContext.scala:219)	
at org.apache.spark.sql.catalyst.analysis.
  Analyzer$CheckResolution$$anonfun$1.
  applyOrElse(Analyzer.scala:80)

then it did not die.
It hung so I killed it with ctrl-c.

If you have any clues on how to debug and run this demo,
please send them.

I'm on ubuntu 14
Here is how spark sees my setup:
Using Scala version 2.10.4 
(Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_60-ea)

Here is the commit I'm working with:

commit 635547f2245845e03e573b7868f29886e8a49599
Author: mmalohlava <michal.malohlava@gmail.com>
Date:   Thu Feb 5 19:23:26 2015 -0800


Thanks,
Dan

</code>
