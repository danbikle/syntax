.q1 Question:
.q2 In Apache Spark how to find max GSPC price?
.tags
  %span.tagtag Tags:
  %a.xtag(href='/tags/apache') apache
  %a.xtag(href='/tags/spark') spark
  %a.xtag(href='/tags/rdd') rdd
  %a.xtag(href='/tags/python') python
  %a.xtag(href='/tags/sparkcontext') sparkcontext
  %a.xtag(href='/tags/sc') sc
  %a.xtag(href='/tags/resilient') resilient
  %a.xtag(href='/tags/distributed') distributed
  %a.xtag(href='/tags/dataset') dataset

%br/
When I learn Spark an early step is to get an RDD.
%br/
%br/
First, the acronym: 'RDD' comes from 'Resilient Distributed Dataset'.
%br/
%br/
I think of it as data in a form which Spark understands.
%br/
%br/
Assuming I have Spark installed in my home directory here:

%pre
  %code.bash ~/spark/

I should see a script here:

%pre
  %code.bash ~/spark/bin/pyspark

I can run it and it and Spark should give me a Python prompt which I can use to interact
with Spark:

%pre
  =render 'spark_max_gspc'

Next, I use wget to get a copy of a CSV file from Yahoo:

%pre
  =render 'spark_max_gspc2'

Then, I use some code to create a Spark RDD from the CSV file:

%pre
  =render 'spark_max_gspc3'
%br/

Next, I use filter() to create a collection of closing-prices.
All it does is remove the header by giving me only lines which have a '0':

%pre
  =render 'spark_max_gspc4'

Then I wrote a series of chained-lambdas which replace whitespace,
split each line at the commas, pick the [4]th field, and finally
convert that field to a float.

I stuff the result of that into a 'closing_prices' variable.

The last step is to feed closing_prices to reduce() which picks the max price:

%pre
  =render 'spark_max_gspc5'




