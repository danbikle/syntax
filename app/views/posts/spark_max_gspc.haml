.q1 Question:
.q2 In Apache Spark how to find max GSPC price?
.tags
  %span.tagtag Tags:
  %a.xtag(href='/tags/apache') apache
  %a.xtag(href='/tags/spark') spark
  %a.xtag(href='/tags/rdd') rdd
  %a.xtag(href='/tags/python') python
  %a.xtag(href='/tags/sparkcontext') sparkcontext
  %a.xtag(href='/tags/sc') sc
  %a.xtag(href='/tags/resilient') resilient
  %a.xtag(href='/tags/distributed') distributed
  %a.xtag(href='/tags/dataset') dataset
  %a.xtag(href='/tags/gspc') gspc
  %a.xtag(href='/tags/map') map
  %a.xtag(href='/tags/reduce') reduce
  %a.xtag(href='/tags/filter') filter
  %a.xtag(href='/tags/mapreduce') mapreduce
  %a.xtag(href='/tags/lambda') lambda
  %a.xtag(href='/tags/max') max

%br/
When I learn Spark an early step is to get an RDD.
%br/
%br/
First, the acronym: 'RDD' comes from 'Resilient Distributed Dataset'.
%br/
%br/
I think of it as data in a form which Spark understands.
%br/
%br/
Assuming I have Spark installed in my home directory here:

%pre
  %code.bash ~/spark/

I should see a script here:

%pre
  %code.bash ~/spark/bin/pyspark

I can run it and it and Spark should give me a Python prompt which I can use to interact
with Spark:

%pre
  =render 'spark_max_gspc'

Next in another shell, I use wget to get a copy of a CSV file from Yahoo:

%pre
  =render 'spark_max_gspc2'

Then, I use some code to create a Spark RDD from the CSV file:

%pre
  =render 'spark_max_gspc3'
%br/

Next, I use filter() to create a collection of closing-prices.
All it does is remove the header by giving me only lines which have a '0':

%pre
  =render 'spark_max_gspc4'

I understand a filter to be a mechanism which filters out false values.

%br/
%br/

It is up to me to generate the false values from the rows of data I want to filter.

%br/
%br/

Spark allows me to think about this in plain English.

%br/
%br/

The plain English behind the above filter is this: if a row has no '0', I should tag it as false.

%br/
%br/

Otherwise I tag it as true.

%br/
%br/

Once I have the plain English cached in my mind, I should translate it into a Python expression.

%br/
%br/

In this example that expression is simple:

%pre
  %code.python '0' in line

Next, I build a lambda from the above expression:


%pre
  %code.python mylam = lambda line: '0' in line

The hard part is over, now I just attach filter(mylam) to the RDD I want to filter.

%br/
%br/

Now that I have an idea of how a filter works,
I write a series of chained-lambdas which replace whitespace,
split each line at the commas, pick the [4]th field, and finally
convert that field to a float.
%br/
%br/

I stuff the result of that into a 'closing_prices' variable.
%br/
%br/

The last step is to feed closing_prices to reduce() which picks the max price:

%pre
  =render 'spark_max_gspc5'

The above value is the same value I found in the data using Pandas:
%br/
%br/

%a(href='/posts/python_pandas_max') python_pandas_max

%br/
%br/
